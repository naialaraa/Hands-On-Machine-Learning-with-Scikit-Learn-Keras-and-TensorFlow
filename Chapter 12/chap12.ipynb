{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 12: Custom Models and Training with TensorFlow\n",
        "\n",
        "Meskipun API tingkat tinggi `tf.keras` sudah mencakup 95% kasus penggunaan, terkadang kita membutuhkan kendali ekstra. Bab ini menyelami API tingkat rendah TensorFlow untuk membuat komponen kustom seperti fungsi aktivasi, regularizer, lapisan, hingga model yang sangat kompleks.\n",
        "\n",
        "## Konten Utama:\n",
        "1. **TensorFlow Dasar**: Manipulasi Tensor dan Variabel.\n",
        "2. **Kustomisasi Komponen Keras**: Membuat fungsi loss, metrik, dan layer buatan sendiri.\n",
        "3. **Autodiff & Gradient Tape**: Mekanisme di balik perhitungan gradien otomatis.\n",
        "4. **Custom Training Loops**: Menjalankan proses pelatihan secara manual tanpa `.fit()`.\n",
        "5. **TF Functions & Graphs**: Mengoptimalkan performa kode Python menjadi grafik komputasi yang efisien."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Menggunakan TensorFlow seperti NumPy\n",
        "\n",
        "Tensor adalah inti dari TensorFlow. Mirip dengan array NumPy, tetapi mendukung akselerasi GPU dan operasi terdistribusi."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Membuat Tensor\n",
        "t = tf.constant([[1., 2., 3.], [4., 5., 6.]])\n",
        "print(\"Tensor:\\n\", t)\n",
        "\n",
        "# Operasi Dasar\n",
        "print(\"Tambah 10:\\n\", t + 10)\n",
        "print(\"Square:\\n\", tf.square(t))\n",
        "\n",
        "# Variabel (Mutable - digunakan untuk bobot model)\n",
        "v = tf.Variable([[1., 2.], [3., 4.]])\n",
        "v.assign(2 * v)\n",
        "v[0, 1].assign(42)\n",
        "print(\"Variable:\\n\", v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Custom Loss Functions\n",
        "\n",
        "Misalkan kita ingin menggunakan **Huber Loss** yang tidak tersedia secara standar atau ingin dikustomisasi threshold-nya."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def huber_fn(y_true, y_pred):\n",
        "    error = y_true - y_pred\n",
        "    is_small_error = tf.abs(error) < 1\n",
        "    squared_loss = tf.square(error) / 2\n",
        "    linear_loss  = tf.abs(error) - 0.5\n",
        "    return tf.where(is_small_error, squared_loss, linear_loss)\n",
        "\n",
        "# Penggunaan dalam Model\n",
        "# model.compile(loss=huber_fn, optimizer=\"nadam\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Custom Layers & Models\n",
        "\n",
        "Kita bisa membuat lapisan kustom dengan melakukan subclassing pada `keras.layers.Layer`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MyDense(tf.keras.layers.Layer):\n",
        "    def __init__(self, units, activation=None, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.activation = tf.keras.activations.get(activation)\n",
        "\n",
        "    def build(self, batch_input_shape):\n",
        "        # Membuat bobot (weights)\n",
        "        self.kernel = self.add_weight(name=\"kernel\", \n",
        "                                      shape=[batch_input_shape[-1], self.units],\n",
        "                                      initializer=\"glorot_normal\")\n",
        "        self.bias = self.add_weight(name=\"bias\", shape=[self.units], \n",
        "                                    initializer=\"zeros\")\n",
        "        super().build(batch_input_shape)\n",
        "\n",
        "    def call(self, X):\n",
        "        return self.activation(X @ self.kernel + self.bias)\n",
        "\n",
        "# Menggunakan layer kustom dalam model\n",
        "model = tf.keras.models.Sequential([\n",
        "    MyDense(30, activation=\"relu\", input_shape=[8]),\n",
        "    MyDense(1)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Perhitungan Gradien dengan Autodiff\n",
        "\n",
        "TensorFlow menggunakan `tf.GradientTape` untuk merekam operasi komputasi sehingga gradien dapat dihitung secara otomatis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def f(w1, w2):\n",
        "    return 3 * w1**2 + 2 * w1 * w2\n",
        "\n",
        "w1, w2 = tf.Variable(5.), tf.Variable(3.)\n",
        "with tf.GradientTape() as tape:\n",
        "    z = f(w1, w2)\n",
        "\n",
        "gradients = tape.gradient(z, [w1, w2])\n",
        "print(\"Gradients [dz/dw1, dz/dw2]:\", gradients)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Custom Training Loops\n",
        "\n",
        "Untuk kendali penuh atas proses update bobot, kita bisa menulis loop pelatihan sendiri tanpa menggunakan `.fit()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Skenario sederhana Custom Training Loop\n",
        "optimizer = tf.keras.optimizers.Nadam(learning_rate=0.01)\n",
        "loss_fn = tf.keras.losses.mean_squared_error\n",
        "\n",
        "def train_step(model, X_batch, y_batch):\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_pred = model(X_batch)\n",
        "        main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
        "        loss = tf.add_n([main_loss] + model.losses)\n",
        "    \n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. TF Functions & Graphs\n",
        "\n",
        "Gunakan dekorator `@tf.function` untuk mengubah fungsi Python biasa menjadi grafik TensorFlow yang sangat cepat."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def tf_cube(x):\n",
        "    return x ** 3\n",
        "\n",
        "print(\"Cube of 2:\", tf_cube(tf.constant(2.0)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Kesimpulan Bab 12\n",
        "Bab ini memberikan dasar bagi pengembang Deep Learning untuk tidak hanya menjadi 'pengguna' library, tetapi juga 'pencipta' arsitektur baru. Dengan memahami Tensor, GradientTape, dan subclassing, Anda siap untuk mengimplementasikan paper penelitian terbaru ke dalam kode."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
