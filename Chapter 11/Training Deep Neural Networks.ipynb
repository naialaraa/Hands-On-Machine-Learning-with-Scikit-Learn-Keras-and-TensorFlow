{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 11: Training Deep Neural Networks\n",
        "\n",
        "Melatih Neural Network yang sangat dalam (DNN) memiliki tantangan tersendiri dibandingkan dengan network dangkal. Bab ini membahas berbagai teknik untuk mengatasi masalah utama dalam Deep Learning agar pelatihan menjadi lebih cepat dan model berkinerja lebih baik.\n",
        "\n",
        "## Masalah Utama dalam Pelatihan DNN:\n",
        "1. **Vanishing/Exploding Gradients**: Gradien menjadi terlalu kecil atau terlalu besar saat backpropagation.\n",
        "2. **Kurangnya Data Berlabel**: Sulit mendapatkan data yang sudah dianotasi untuk tugas kompleks.\n",
        "3. **Pelatihan Lambat**: DNN besar membutuhkan waktu lama untuk konvergen.\n",
        "4. **Overfitting**: Model dengan jutaan parameter sangat rentan menghafal data latih."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Mengatasi Vanishing & Exploding Gradients\n",
        "\n",
        "Salah satu solusi utama adalah menggunakan teknik inisialisasi bobot yang tepat sesuai dengan fungsi aktivasi yang digunakan.\n",
        "\n",
        "### Inisialisasi He\n",
        "Digunakan terutama untuk fungsi aktivasi **ReLU** dan variannya (Leaky ReLU, ELU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Contoh penggunaan He Initialization di Keras\n",
        "layer = keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Fungsi Aktivasi Non-Saturasi\n",
        "\n",
        "Fungsi Sigmoid cenderung mengalami saturasi (gradien mendekati 0) pada nilai input yang ekstrem. Alternatif yang lebih baik meliputi:\n",
        "- **Leaky ReLU**: Memiliki slope kecil pada nilai negatif untuk mencegah 'dead neurons'.\n",
        "- **ELU (Exponential Linear Unit)**: Lebih halus di sekitar nol dan seringkali konvergen lebih cepat.\n",
        "- **SELU**: Dapat menormalkan network secara mandiri (self-normalization) jika arsitekturnya tepat."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Menggunakan Leaky ReLU\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(10, kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.LeakyReLU(alpha=0.2)\n",
        "])\n",
        "\n",
        "# Menggunakan SELU untuk Self-Normalization\n",
        "layer_selu = keras.layers.Dense(10, activation=\"selu\", kernel_initializer=\"lecun_normal\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Batch Normalization\n",
        "\n",
        "Teknik ini menormalkan input di setiap lapisan selama pelatihan, yang secara drastis mengurangi masalah gradien dan memungkinkan penggunaan learning rate yang lebih tinggi."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Transfer Learning (Reusing Layers)\n",
        "\n",
        "Daripada melatih dari awal, kita bisa menggunakan lapisan bawah dari model yang sudah terlatih untuk tugas yang mirip."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Misalkan kita memuat model A\n",
        "# model_A = keras.models.load_model(\"my_model_A.h5\")\n",
        "\n",
        "# Membuat model B berdasarkan model A tanpa lapisan output terakhir\n",
        "# model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
        "# model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "# Membekukan (Freezing) lapisan yang digunakan kembali\n",
        "# for layer in model_B_on_A.layers[:-1]:\n",
        "#     layer.trainable = False\n",
        "\n",
        "# Jangan lupa untuk compile setelah mengubah status trainable\n",
        "# model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Optimizer Cepat\n",
        "\n",
        "Beberapa optimizer populer yang lebih cepat dari Stochastic Gradient Descent (SGD) standar:\n",
        "- **Momentum Optimization**: Mempercepat gradien di arah yang benar (seperti bola menggelinding).\n",
        "- **Nesterov Accelerated Gradient (NAG)**: Variasi momentum yang lebih cerdas dalam 'melihat ke depan'.\n",
        "- **RMSProp**: Menyesuaikan learning rate secara adaptif.\n",
        "- **Adam**: Menggabungkan ide Momentum dan RMSProp."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Menggunakan Adam Optimizer\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Regularisasi untuk Mencegah Overfitting\n",
        "\n",
        "### Dropout\n",
        "Teknik di mana setiap neuron memiliki probabilitas (misal 20%) untuk 'dimatikan' selama iterasi pelatihan. Ini memaksa network untuk tidak terlalu bergantung pada neuron spesifik."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dropout(rate=0.2),\n",
        "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dropout(rate=0.2),\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dropout(rate=0.2),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Kesimpulan Praktis\n",
        "Berdasarkan bab ini, konfigurasi default yang disarankan untuk DNN umum adalah:\n",
        "- **Inisialisasi**: He Initialization.\n",
        "- **Aktivasi**: ELU (atau ReLU jika mengutamakan kecepatan).\n",
        "- **Normalisasi**: Batch Normalization (terutama untuk network dalam).\n",
        "- **Regularisasi**: Early Stopping + Dropout.\n",
        "- **Optimizer**: Adam atau Nadam.\n",
        "- **Learning Rate**: Menggunakan scheduler seperti 1cycle."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
