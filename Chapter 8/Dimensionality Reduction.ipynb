{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 8: Dimensionality Reduction\n",
        "\n",
        "Pada banyak permasalahan Machine Learning di dunia nyata, data yang digunakan memiliki **jumlah fitur yang sangat besar**, bahkan bisa mencapai ribuan atau jutaan dimensi.\n",
        "\n",
        "Dimensionality Reduction merupakan sekumpulan teknik yang bertujuan untuk **mengurangi jumlah fitur** tanpa kehilangan terlalu banyak informasi penting.\n",
        "\n",
        "Chapter ini membahas mengapa dimensionality reduction diperlukan, apa saja pendekatan utamanya, serta algoritma populer seperti **PCA, Kernel PCA, dan LLE**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dimensionality reduction memiliki beberapa manfaat utama:\n",
        "- mempercepat proses training model,\n",
        "- mengurangi risiko overfitting,\n",
        "- mengatasi masalah *curse of dimensionality*,\n",
        "- serta mempermudah visualisasi data berdimensi tinggi.\n",
        "\n",
        "Namun, teknik ini juga memiliki trade-off karena dapat menyebabkan **kehilangan informasi**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. The Curse of Dimensionality\n",
        "\n",
        "Istilah **curse of dimensionality** mengacu pada berbagai fenomena yang muncul ketika jumlah dimensi data menjadi sangat besar.\n",
        "\n",
        "Dalam ruang berdimensi tinggi, intuisi manusia sering kali gagal karena sifat geometris ruang tersebut sangat berbeda dibandingkan ruang 2D atau 3D yang biasa kita bayangkan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Sparsity in High-Dimensional Space\n",
        "\n",
        "Ketika jumlah dimensi meningkat:\n",
        "- data menjadi sangat **jarang (sparse)**,\n",
        "- jarak antar titik data cenderung semakin besar,\n",
        "- instance baru sering kali jauh dari semua data training.\n",
        "\n",
        "Akibatnya, model Machine Learning menjadi lebih sulit untuk melakukan generalisasi yang baik."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sebagai contoh intuitif:\n",
        "- pada ruang 2D, sebagian besar titik berada di bagian tengah,\n",
        "- tetapi pada ruang berdimensi sangat tinggi, sebagian besar titik berada **sangat dekat dengan batas ruang**.\n",
        "\n",
        "Fenomena ini membuat banyak algoritma Machine Learning menjadi tidak stabil dan rentan terhadap overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Dampak Curse of Dimensionality\n",
        "\n",
        "Beberapa dampak utama curse of dimensionality antara lain:\n",
        "- kebutuhan data training meningkat secara eksponensial,\n",
        "- model menjadi lebih kompleks dan sulit dilatih,\n",
        "- performa prediksi menurun karena ekstrapolasi yang berlebihan.\n",
        "\n",
        "Oleh karena itu, dimensionality reduction sering kali menjadi solusi praktis untuk membuat masalah Machine Learning lebih terkelola."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Main Approaches for Dimensionality Reduction\n",
        "\n",
        "Secara umum, teknik dimensionality reduction dapat dibagi menjadi dua pendekatan utama:\n",
        "- **Projection** (proyeksi ke ruang berdimensi lebih rendah),\n",
        "- **Manifold Learning** (pembelajaran struktur manifold data).\n",
        "\n",
        "Kedua pendekatan ini memiliki tujuan yang sama, yaitu mengurangi dimensi data, tetapi berangkat dari asumsi yang berbeda tentang struktur data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Projection\n",
        "\n",
        "Pendekatan **projection** berasumsi bahwa data berdimensi tinggi sebenarnya terletak dekat dengan sebuah **subspace berdimensi lebih rendah**.\n",
        "\n",
        "Dengan kata lain, meskipun data memiliki banyak fitur, sebagian besar variansnya dapat dijelaskan oleh kombinasi linier dari beberapa fitur saja."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sebagai contoh:\n",
        "- data 3D yang sebenarnya hampir berada pada sebuah bidang 2D,\n",
        "- data 100D yang sebenarnya efektif berada pada subspace 10D.\n",
        "\n",
        "Teknik projection bertujuan untuk menemukan subspace tersebut dan memproyeksikan data ke dalamnya."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pendekatan ini sangat efektif ketika:\n",
        "- fitur-fitur saling berkorelasi,\n",
        "- terdapat redundansi informasi,\n",
        "- struktur data relatif linier.\n",
        "\n",
        "Algoritma paling populer dalam kategori ini adalah **Principal Component Analysis (PCA)**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Manifold Learning\n",
        "\n",
        "Berbeda dengan projection, **manifold learning** berasumsi bahwa data berdimensi tinggi terletak pada sebuah **manifold non-linear** berdimensi lebih rendah.\n",
        "\n",
        "Manifold dapat dipahami sebagai permukaan melengkung yang tertanam di ruang berdimensi tinggi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sebagai ilustrasi:\n",
        "- permukaan kertas yang dilipat dan diletakkan di ruang 3D,\n",
        "- data wajah dengan berbagai pose dan ekspresi.\n",
        "\n",
        "Meskipun data terlihat kompleks di ruang asli, struktur intrinsiknya mungkin sederhana jika dilihat pada manifold yang tepat."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Manifold learning berfokus pada:\n",
        "- menjaga hubungan lokal antar titik data,\n",
        "- mempertahankan struktur geometris non-linear,\n",
        "- menghasilkan representasi berdimensi rendah yang lebih bermakna.\n",
        "\n",
        "Contoh algoritma manifold learning antara lain **Locally Linear Embedding (LLE)** dan **t-SNE**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Kapan Menggunakan Projection atau Manifold Learning?\n",
        "\n",
        "Secara umum:\n",
        "- **Projection** cocok untuk preprocessing sebelum training model Machine Learning,\n",
        "- **Manifold learning** sering digunakan untuk visualisasi dan eksplorasi data.\n",
        "\n",
        "Manifold learning jarang digunakan sebagai preprocessing untuk training karena sering kali tidak mempertahankan jarak global dengan baik."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Principal Component Analysis (PCA)\n",
        "\n",
        "**Principal Component Analysis (PCA)** merupakan teknik dimensionality reduction paling populer dan paling banyak digunakan.\n",
        "\n",
        "PCA termasuk dalam pendekatan **projection**, dan bertujuan untuk memproyeksikan data ke subspace berdimensi lebih rendah dengan **kehilangan informasi seminimal mungkin**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Intuisi Dasar PCA\n",
        "\n",
        "Ide utama PCA adalah mencari **arah (axis)** di mana data memiliki **variansi terbesar**.\n",
        "\n",
        "Arah ini disebut sebagai **principal components**.\n",
        "\n",
        "Dengan memproyeksikan data ke beberapa principal components pertama, kita dapat mempertahankan sebagian besar informasi penting dari data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Secara intuitif:\n",
        "- principal component pertama adalah arah dengan variansi terbesar,\n",
        "- principal component kedua adalah arah dengan variansi terbesar kedua dan ortogonal terhadap yang pertama,\n",
        "- dan seterusnya.\n",
        "\n",
        "PCA selalu menghasilkan sumbu-sumbu baru yang **saling ortogonal**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Preserving Variance\n",
        "\n",
        "Tujuan utama PCA bukan memaksimalkan akurasi klasifikasi atau regresi secara langsung, melainkan **memaksimalkan variansi yang dipertahankan** pada ruang berdimensi lebih rendah.\n",
        "\n",
        "Dengan mempertahankan variansi yang besar, PCA berharap struktur penting dalam data tetap terjaga."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sebagai contoh:\n",
        "- jika 95% variansi data dapat dijelaskan oleh 2 komponen utama,\n",
        "- maka memproyeksikan data ke 2 dimensi tersebut biasanya sudah cukup representatif.\n",
        "\n",
        "Sisa variansi sering kali dianggap sebagai noise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 PCA vs Feature Selection\n",
        "\n",
        "Penting untuk membedakan PCA dengan **feature selection**:\n",
        "\n",
        "- PCA membuat fitur baru berupa kombinasi linier dari fitur asli,\n",
        "- feature selection hanya memilih subset fitur asli.\n",
        "\n",
        "Akibatnya, hasil PCA sering kali lebih sulit diinterpretasikan, tetapi lebih efektif dalam mengurangi dimensi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.4 Keterbatasan PCA\n",
        "\n",
        "Meskipun sangat berguna, PCA memiliki beberapa keterbatasan:\n",
        "- hanya mampu menangkap hubungan linier,\n",
        "- sensitif terhadap skala fitur,\n",
        "- tidak mempertimbangkan label (unsupervised).\n",
        "\n",
        "Oleh karena itu, **standardisasi fitur** biasanya dilakukan sebelum menerapkan PCA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. PCA with NumPy and Scikit-Learn\n",
        "\n",
        "Setelah memahami intuisi PCA, langkah berikutnya adalah melihat bagaimana PCA diimplementasikan secara praktis.\n",
        "\n",
        "Pada bagian ini, kita akan menerapkan PCA menggunakan:\n",
        "- pendekatan manual dengan NumPy (untuk memahami mekanismenya),\n",
        "- implementasi resmi PCA dari Scikit-Learn (untuk penggunaan praktis)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 PCA dengan NumPy (Pendekatan Konseptual)\n",
        "\n",
        "Secara matematis, PCA dapat dihitung menggunakan **Singular Value Decomposition (SVD)**.\n",
        "\n",
        "Langkah umumnya adalah:\n",
        "1. menstandarisasi data,\n",
        "2. menghitung SVD,\n",
        "3. memilih beberapa komponen utama,\n",
        "4. memproyeksikan data ke ruang baru."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# contoh data sederhana\n",
        "X = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "\n",
        "# centering data\n",
        "X_centered = X - X.mean(axis=0)\n",
        "\n",
        "# Singular Value Decomposition\n",
        "U, S, Vt = np.linalg.svd(X_centered)\n",
        "\n",
        "Vt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Matriks `Vt` berisi **principal components**, yaitu arah baru tempat data memiliki variansi terbesar.\n",
        "\n",
        "Baris pertama dari `Vt` merupakan principal component pertama, diikuti oleh komponen berikutnya."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Projecting Data onto Principal Components\n",
        "\n",
        "Setelah principal components diperoleh, data dapat diproyeksikan ke subspace berdimensi lebih rendah."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# memilih 1 komponen utama\n",
        "W2 = Vt[:1].T\n",
        "\n",
        "# proyeksi data\n",
        "X_pca = X_centered.dot(W2)\n",
        "X_pca"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hasil proyeksi ini menunjukkan representasi data dalam satu dimensi yang mempertahankan variansi terbesar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 PCA dengan Scikit-Learn\n",
        "\n",
        "Dalam praktik nyata, kita jarang mengimplementasikan PCA secara manual.\n",
        "\n",
        "Scikit-Learn menyediakan class `PCA` yang efisien dan mudah digunakan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_pca_sklearn = pca.fit_transform(X)\n",
        "X_pca_sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Implementasi PCA dari Scikit-Learn secara internal menggunakan SVD, tetapi menyediakan antarmuka yang jauh lebih praktis.\n",
        "\n",
        "Hasil proyeksi konsisten dengan pendekatan manual menggunakan NumPy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Choosing the Right Number of Dimensions\n",
        "\n",
        "Salah satu keputusan penting dalam menggunakan PCA adalah menentukan **berapa banyak principal components** yang harus dipertahankan.\n",
        "\n",
        "Terlalu sedikit komponen dapat menyebabkan hilangnya informasi penting, sedangkan terlalu banyak komponen dapat mengurangi manfaat dimensionality reduction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 Explained Variance Ratio\n",
        "\n",
        "PCA menyediakan metrik bernama **explained variance ratio**, yang menunjukkan proporsi variansi data yang dijelaskan oleh setiap principal component.\n",
        "\n",
        "Dengan melihat nilai ini, kita dapat menilai seberapa besar kontribusi masing-masing komponen terhadap keseluruhan variansi data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pca = PCA()\n",
        "pca.fit(X)\n",
        "\n",
        "pca.explained_variance_ratio_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Array di atas menunjukkan berapa persen variansi yang dijelaskan oleh setiap principal component, mulai dari yang paling penting.\n",
        "\n",
        "Biasanya, beberapa komponen pertama sudah menjelaskan sebagian besar variansi data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Cumulative Explained Variance\n",
        "\n",
        "Untuk menentukan jumlah komponen yang optimal, kita sering melihat **cumulative explained variance**, yaitu akumulasi variansi yang dijelaskan oleh beberapa komponen pertama."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.cumsum(pca.explained_variance_ratio_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sebagai contoh:\n",
        "- jika 95% variansi tercapai dengan 2 komponen,\n",
        "- maka mempertahankan 2 komponen tersebut sering kali sudah cukup.\n",
        "\n",
        "Pendekatan ini umum digunakan dalam praktik."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3 Choosing Components Automatically\n",
        "\n",
        "Scikit-Learn memungkinkan kita memilih jumlah komponen berdasarkan target explained variance secara langsung."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pca_95 = PCA(n_components=0.95)\n",
        "X_reduced = pca_95.fit_transform(X)\n",
        "\n",
        "X_reduced.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dengan pendekatan ini, PCA secara otomatis memilih jumlah komponen minimum yang mempertahankan setidaknya 95% variansi data.\n",
        "\n",
        "Pendekatan ini sangat praktis ketika jumlah fitur awal sangat besar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. PCA Variants\n",
        "\n",
        "Ketika dataset menjadi sangat besar, PCA standar dapat menjadi mahal secara komputasi.\n",
        "\n",
        "Untuk mengatasi hal ini, tersedia beberapa **varian PCA** yang dirancang agar lebih efisien dan skalabel, yaitu **Randomized PCA** dan **Incremental PCA**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 Randomized PCA\n",
        "\n",
        "Randomized PCA menggunakan pendekatan probabilistik untuk memperkirakan principal components.\n",
        "\n",
        "Pendekatan ini sangat cepat dan cocok digunakan ketika:\n",
        "- jumlah fitur sangat besar,\n",
        "- hanya beberapa komponen utama yang dibutuhkan.\n",
        "\n",
        "Dalam banyak kasus, Randomized PCA menghasilkan hasil yang hampir sama dengan PCA standar, tetapi dengan waktu komputasi yang jauh lebih singkat."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Scikit-Learn secara otomatis menggunakan Randomized PCA ketika:\n",
        "- jumlah komponen jauh lebih kecil dari jumlah fitur,\n",
        "- solver diatur ke `svd_solver=\"randomized\"`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pca_randomized = PCA(n_components=2, svd_solver=\"randomized\", random_state=42)\n",
        "X_pca_randomized = pca_randomized.fit_transform(X)\n",
        "X_pca_randomized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hasil Randomized PCA ini secara praktis sangat mirip dengan PCA standar, terutama ketika jumlah komponen yang diambil relatif kecil."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Incremental PCA\n",
        "\n",
        "**Incremental PCA** dirancang untuk menangani dataset yang **tidak muat di memori**.\n",
        "\n",
        "Algoritma ini memproses data secara bertahap (mini-batch), sehingga cocok untuk data streaming atau dataset berukuran sangat besar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Incremental PCA bekerja dengan:\n",
        "- memproses sebagian data dalam satu waktu,\n",
        "- memperbarui principal components secara bertahap,\n",
        "- tanpa perlu memuat seluruh dataset sekaligus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.decomposition import IncrementalPCA\n",
        "\n",
        "inc_pca = IncrementalPCA(n_components=2)\n",
        "\n",
        "for batch in np.array_split(X, 3):\n",
        "    inc_pca.partial_fit(batch)\n",
        "\n",
        "X_pca_incremental = inc_pca.transform(X)\n",
        "X_pca_incremental"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Incremental PCA memungkinkan PCA diterapkan pada dataset besar tanpa mengorbankan terlalu banyak akurasi.\n",
        "\n",
        "Pendekatan ini sangat berguna dalam sistem Machine Learning berskala besar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Kernel PCA\n",
        "\n",
        "PCA standar hanya mampu menangkap hubungan **linier** antar fitur.\n",
        "\n",
        "Ketika struktur data bersifat non-linear, PCA biasa sering gagal merepresentasikan data dengan baik.\n",
        "\n",
        "**Kernel PCA** mengatasi keterbatasan ini dengan memanfaatkan *kernel trick* untuk melakukan PCA pada ruang fitur berdimensi lebih tinggi secara implisit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.1 Intuisi Kernel Trick\n",
        "\n",
        "Alih-alih memproyeksikan data secara eksplisit ke ruang berdimensi tinggi, kernel trick menghitung **kemiripan antar titik data** menggunakan fungsi kernel.\n",
        "\n",
        "Dengan cara ini, hubungan non-linear di ruang asli dapat menjadi linier di ruang fitur hasil transformasi kernel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Beberapa kernel yang umum digunakan dalam Kernel PCA:\n",
        "- **Linear**: setara dengan PCA biasa,\n",
        "- **Polynomial**: menangkap hubungan polinomial,\n",
        "- **RBF (Gaussian)**: sangat fleksibel untuk pola non-linear kompleks,\n",
        "- **Sigmoid**: mirip dengan fungsi aktivasi pada neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.2 Kernel PCA dengan Scikit-Learn\n",
        "\n",
        "Scikit-Learn menyediakan implementasi Kernel PCA melalui class `KernelPCA`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.decomposition import KernelPCA\n",
        "\n",
        "kpca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=0.04)\n",
        "X_kpca = kpca.fit_transform(X)\n",
        "X_kpca"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pada contoh di atas, kernel RBF digunakan untuk menangkap struktur non-linear pada data.\n",
        "\n",
        "Parameter `gamma` mengontrol lebar kernel dan sangat mempengaruhi hasil transformasi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.3 Kernel PCA sebagai Preprocessing\n",
        "\n",
        "Kernel PCA sering digunakan sebagai tahap preprocessing sebelum algoritma Machine Learning lain, seperti SVM atau Logistic Regression.\n",
        "\n",
        "Namun, karena transformasi kernel bersifat non-linear dan tidak selalu invertible, interpretasi fitur hasil Kernel PCA menjadi lebih sulit dibandingkan PCA biasa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Locally Linear Embedding (LLE)\n",
        "\n",
        "**Locally Linear Embedding (LLE)** merupakan salah satu algoritma **manifold learning** yang bertujuan menemukan representasi berdimensi rendah dari data berdimensi tinggi dengan mempertahankan **struktur lokal**.\n",
        "\n",
        "Berbeda dengan PCA yang bersifat linier, LLE dirancang untuk menangani struktur data **non-linear**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.1 Intuisi Dasar LLE\n",
        "\n",
        "Ide utama LLE adalah asumsi bahwa setiap titik data dapat direpresentasikan sebagai **kombinasi linier dari tetangga terdekatnya**.\n",
        "\n",
        "Jika hubungan lokal ini dapat dipertahankan dalam ruang berdimensi rendah, maka struktur manifold data juga akan tetap terjaga."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Secara garis besar, algoritma LLE bekerja dalam tiga tahap:\n",
        "1. untuk setiap titik data, mencari *k-nearest neighbors*,\n",
        "2. menghitung bobot yang merekonstruksi setiap titik dari tetangganya,\n",
        "3. mencari embedding berdimensi rendah yang mempertahankan bobot tersebut.\n",
        "\n",
        "Dengan cara ini, LLE fokus pada **hubungan lokal**, bukan jarak global."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.2 Karakteristik LLE\n",
        "\n",
        "Beberapa karakteristik penting dari LLE antara lain:\n",
        "- sangat efektif untuk visualisasi data berdimensi tinggi,\n",
        "- mampu menangkap struktur manifold non-linear,\n",
        "- sensitif terhadap noise dan pemilihan jumlah tetangga (`n_neighbors`).\n",
        "\n",
        "Karena fokus pada struktur lokal, LLE jarang digunakan sebagai preprocessing untuk training model Machine Learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.3 LLE dengan Scikit-Learn\n",
        "\n",
        "Scikit-Learn menyediakan implementasi LLE melalui class `LocallyLinearEmbedding`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.manifold import LocallyLinearEmbedding\n",
        "\n",
        "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)\n",
        "X_lle = lle.fit_transform(X)\n",
        "X_lle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hasil embedding LLE di atas menunjukkan representasi data dalam dua dimensi yang mempertahankan hubungan lokal antar titik.\n",
        "\n",
        "Teknik ini sering digunakan untuk eksplorasi dan visualisasi struktur data kompleks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Other Dimensionality Reduction Techniques\n",
        "\n",
        "Selain PCA dan LLE, terdapat berbagai teknik dimensionality reduction lain yang sering digunakan dalam praktik, tergantung pada tujuan dan karakteristik data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.1 Multidimensional Scaling (MDS)\n",
        "\n",
        "**Multidimensional Scaling (MDS)** bertujuan untuk merepresentasikan data dalam ruang berdimensi rendah dengan mempertahankan **jarak antar titik data**.\n",
        "\n",
        "MDS sering digunakan untuk visualisasi data, terutama ketika informasi jarak antar instance sangat penting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.2 t-SNE (t-Distributed Stochastic Neighbor Embedding)\n",
        "\n",
        "**t-SNE** merupakan teknik manifold learning yang sangat populer untuk visualisasi data berdimensi tinggi.\n",
        "\n",
        "Algoritma ini sangat baik dalam mempertahankan struktur lokal, sehingga sering digunakan untuk memvisualisasikan clustering data.\n",
        "\n",
        "Namun, t-SNE:\n",
        "- mahal secara komputasi,\n",
        "- tidak cocok sebagai preprocessing sebelum training model,\n",
        "- hasilnya sensitif terhadap hyperparameter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.3 Isomap\n",
        "\n",
        "**Isomap** merupakan metode manifold learning yang mempertahankan **jarak geodesik** antar titik data pada manifold.\n",
        "\n",
        "Isomap cocok untuk data dengan struktur non-linear yang relatif halus, tetapi kurang robust terhadap noise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Secara umum, teknik manifold learning seperti LLE, Isomap, dan t-SNE lebih sering digunakan untuk **eksplorasi dan visualisasi data** dibandingkan sebagai tahap preprocessing Machine Learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Closing Summary (Chapter 8)\n",
        "\n",
        "Chapter 8 membahas berbagai teknik **Dimensionality Reduction** yang bertujuan mengurangi kompleksitas data tanpa kehilangan terlalu banyak informasi penting.\n",
        "\n",
        "Konsep utama yang dipelajari meliputi:\n",
        "- curse of dimensionality dan dampaknya,\n",
        "- pendekatan projection dan manifold learning,\n",
        "- Principal Component Analysis (PCA) dan variannya,\n",
        "- Kernel PCA untuk struktur non-linear,\n",
        "- serta teknik manifold learning seperti LLE.\n",
        "\n",
        "Dimensionality reduction merupakan alat penting dalam Machine Learning modern, baik untuk meningkatkan efisiensi model maupun untuk memahami struktur data kompleks.\n",
        "\n",
        "Pemahaman chapter ini menjadi fondasi untuk mempelajari topik lanjutan seperti **unsupervised learning**, **clustering**, dan **representation learning** pada chapter berikutnya."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
