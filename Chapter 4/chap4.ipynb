{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 4: Training Models\n",
        "\n",
        "Pada chapter ini, pembahasan Machine Learning berfokus pada **bagaimana sebuah model dilatih secara matematis dan algoritmik**. Jika pada chapter sebelumnya kita banyak menggunakan model sebagai *black box*, maka chapter ini mengajak pembaca untuk melihat apa yang sebenarnya terjadi *di balik layar*.\n",
        "\n",
        "Pemahaman mengenai proses training sangat penting karena membantu kita:\n",
        "- memilih model yang tepat untuk suatu permasalahan,\n",
        "- menentukan algoritma optimisasi yang sesuai,\n",
        "- melakukan debugging ketika model tidak bekerja dengan baik,\n",
        "- serta memahami konsep yang menjadi dasar neural networks.\n",
        "\n",
        "Chapter ini membahas beberapa topik utama, antara lain:\n",
        "- Linear Regression dan cara melatihnya,\n",
        "- Gradient Descent dan variasinya,\n",
        "- Polynomial Regression,\n",
        "- biasâ€“variance trade-off,\n",
        "- regularization,\n",
        "- hingga Logistic Regression dan Softmax Regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Linear Regression\n",
        "\n",
        "Linear Regression merupakan salah satu model Machine Learning paling sederhana dan paling fundamental. Model ini mengasumsikan bahwa hubungan antara fitur input dan target dapat direpresentasikan sebagai **kombinasi linear**.\n",
        "\n",
        "Secara intuitif, Linear Regression mencoba mencari garis (atau bidang, atau hiperbidang) yang paling sesuai dengan data, sehingga kesalahan prediksi terhadap data training menjadi sekecil mungkin."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Model Linear Regression\n",
        "\n",
        "Untuk sebuah instance dengan beberapa fitur, Linear Regression memprediksi nilai target dengan menghitung **jumlah tertimbang dari seluruh fitur**, kemudian menambahkan sebuah konstanta yang disebut *bias* atau *intercept*.\n",
        "\n",
        "Secara matematis, model Linear Regression dapat dituliskan sebagai:\n",
        "\n",
        "\\[ \\hat{y} = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_n x_n \\]\n",
        "\n",
        "Di mana:\n",
        "- \\( \\hat{y} \\) adalah nilai prediksi,\n",
        "- \\( x_i \\) adalah nilai fitur ke-i,\n",
        "- \\( \\theta_i \\) adalah parameter model yang dipelajari dari data.\n",
        "\n",
        "Model ini juga dapat dituliskan dalam bentuk vektorisasi untuk memudahkan komputasi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Generating Linear Data\n",
        "\n",
        "Untuk memahami bagaimana Linear Regression bekerja dan bagaimana model dilatih, kita akan membuat dataset sintetis sederhana yang mengikuti hubungan linear, kemudian menambahkan noise.\n",
        "\n",
        "Dataset ini akan digunakan untuk mendemonstrasikan berbagai metode training pada Linear Regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "X = 2 * np.random.rand(100, 1)\n",
        "y = 4 + 3 * X + np.random.randn(100, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data di atas dibuat menggunakan fungsi linear \\( y = 4 + 3x \\) dengan tambahan *Gaussian noise*. Noise ini mencerminkan kondisi dunia nyata, di mana data jarang bersifat sempurna."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Training Linear Regression Using the Normal Equation\n",
        "\n",
        "Salah satu cara untuk melatih model Linear Regression adalah dengan menggunakan **Normal Equation**. Pendekatan ini mencari solusi optimal secara langsung melalui perhitungan matematis, tanpa memerlukan iterasi bertahap.\n",
        "\n",
        "Normal Equation memberikan solusi parameter \\( \\theta \\) yang meminimalkan *Mean Squared Error* secara analitik."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Normal Equation\n",
        "\n",
        "Secara matematis, solusi Linear Regression menggunakan Normal Equation dapat dituliskan sebagai:\n",
        "\n",
        "\\[ \\theta = (X^T X)^{-1} X^T y \\]\n",
        "\n",
        "Rumus ini menghitung parameter optimal dalam satu langkah. Namun, pendekatan ini memiliki keterbatasan karena kompleksitas komputasinya meningkat drastis ketika jumlah fitur sangat besar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_b = np.c_[np.ones((100, 1)), X]\n",
        "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
        "theta_best"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hasil di atas menunjukkan nilai parameter \\( \\theta_0 \\) dan \\( \\theta_1 \\) yang mendekati nilai asli yang digunakan untuk menghasilkan data, yaitu 4 dan 3.\n",
        "\n",
        "Perbedaan kecil terjadi karena adanya noise pada data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Making Predictions\n",
        "\n",
        "Setelah parameter model diperoleh, kita dapat menggunakan model Linear Regression untuk melakukan prediksi pada data baru."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_new = np.array([[0], [2]])\n",
        "X_new_b = np.c_[np.ones((2, 1)), X_new]\n",
        "y_predict = X_new_b.dot(theta_best)\n",
        "y_predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prediksi di atas menunjukkan nilai target yang diperkirakan oleh model untuk input baru. Model menggunakan parameter yang telah dipelajari untuk melakukan generalisasi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Linear Regression Using Scikit-Learn\n",
        "\n",
        "Pada praktiknya, kita jarang menghitung Normal Equation secara manual. Library seperti **Scikit-Learn** menyediakan implementasi Linear Regression yang efisien dan stabil secara numerik.\n",
        "\n",
        "Implementasi ini secara internal menggunakan teknik optimisasi yang setara, tanpa mengharuskan kita menghitung invers matriks secara eksplisit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X, y)\n",
        "lin_reg.intercept_, lin_reg.coef_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nilai intercept dan koefisien yang dihasilkan oleh Scikit-Learn sangat mendekati hasil dari Normal Equation, membuktikan bahwa kedua pendekatan menghasilkan solusi yang sama."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Predictions Using Scikit-Learn\n",
        "\n",
        "Model Linear Regression dari Scikit-Learn dapat langsung digunakan untuk melakukan prediksi tanpa perlu menambahkan bias secara manual."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lin_reg.predict(X_new)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hasil prediksi konsisten dengan prediksi sebelumnya, menunjukkan bahwa Scikit-Learn menangani detail implementasi secara otomatis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Gradient Descent\n",
        "\n",
        "Ketika jumlah fitur sangat besar, pendekatan Normal Equation menjadi tidak efisien karena membutuhkan invers matriks. Untuk kondisi ini, digunakan metode optimisasi iteratif yang disebut **Gradient Descent**.\n",
        "\n",
        "Gradient Descent bekerja dengan menyesuaikan parameter model secara bertahap, bergerak ke arah yang menurunkan nilai fungsi biaya (*cost function*) hingga mencapai minimum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 The Cost Function\n",
        "\n",
        "Untuk Linear Regression, fungsi biaya yang umum digunakan adalah **Mean Squared Error (MSE)**. Fungsi ini mengukur rata-rata kuadrat selisih antara nilai prediksi dan nilai aktual.\n",
        "\n",
        "Gradient Descent bertujuan meminimalkan nilai MSE dengan memperbarui parameter \\( \\theta \\) secara iteratif."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Batch Gradient Descent\n",
        "\n",
        "Pada **Batch Gradient Descent**, gradien dihitung menggunakan seluruh data training pada setiap iterasi. Pendekatan ini memberikan arah gradien yang stabil, namun bisa menjadi lambat untuk dataset yang besar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eta = 0.1\n",
        "n_iterations = 1000\n",
        "m = 100\n",
        "\n",
        "theta = np.random.randn(2, 1)\n",
        "\n",
        "for iteration in range(n_iterations):\n",
        "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
        "    theta = theta - eta * gradients\n",
        "\n",
        "theta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hasil parameter yang diperoleh melalui Batch Gradient Descent mendekati hasil Normal Equation. Hal ini menunjukkan bahwa metode iteratif mampu mencapai solusi optimal jika *learning rate* dipilih dengan tepat."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3 Learning Rate\n",
        "\n",
        "*Learning rate* (\\( \\eta \\)) menentukan seberapa besar langkah pembaruan parameter pada setiap iterasi.\n",
        "\n",
        "- Learning rate terlalu kecil menyebabkan konvergensi lambat\n",
        "- Learning rate terlalu besar dapat menyebabkan divergensi\n",
        "\n",
        "Pemilihan learning rate yang tepat sangat krusial dalam proses training model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Secara konseptual, perilaku Gradient Descent dengan berbagai nilai learning rate sering divisualisasikan untuk menunjukkan perbedaan antara konvergensi yang stabil dan divergensi, seperti yang digambarkan pada ilustrasi di buku."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Stochastic Gradient Descent (SGD)\n",
        "\n",
        "Berbeda dengan Batch Gradient Descent yang menggunakan seluruh data training pada setiap iterasi, **Stochastic Gradient Descent (SGD)** memperbarui parameter model menggunakan **satu instance data secara acak** pada setiap langkah.\n",
        "\n",
        "Pendekatan ini membuat proses training menjadi jauh lebih cepat, terutama pada dataset yang sangat besar. Namun, karena arah gradien menjadi lebih berisik (*noisy*), proses konvergensinya tidak selalu stabil."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 Karakteristik SGD\n",
        "\n",
        "Beberapa karakteristik utama dari Stochastic Gradient Descent adalah:\n",
        "- Pembaruan parameter sangat cepat\n",
        "- Cocok untuk dataset berskala besar atau *online learning*\n",
        "- Jalur menuju minimum fungsi biaya berosilasi\n",
        "- Sulit mencapai minimum absolut, tetapi dapat mendekati minimum global\n",
        "\n",
        "Osilasi ini justru membantu SGD keluar dari minimum lokal yang buruk."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Learning Schedule\n",
        "\n",
        "Untuk mengatasi osilasi pada SGD, sering digunakan **learning schedule**, yaitu strategi menurunkan learning rate secara bertahap selama training.\n",
        "\n",
        "Dengan learning rate yang semakin kecil, langkah pembaruan parameter menjadi lebih halus ketika model mendekati solusi optimal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_epochs = 50\n",
        "t0, t1 = 5, 50\n",
        "\n",
        "def learning_schedule(t):\n",
        "    return t0 / (t + t1)\n",
        "\n",
        "theta = np.random.randn(2, 1)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for i in range(m):\n",
        "        random_index = np.random.randint(m)\n",
        "        xi = X_b[random_index:random_index+1]\n",
        "        yi = y[random_index:random_index+1]\n",
        "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
        "        eta = learning_schedule(epoch * m + i)\n",
        "        theta = theta - eta * gradients\n",
        "\n",
        "theta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hasil parameter dari SGD mendekati solusi optimal, meskipun jalur konvergensinya tidak sehalus Batch Gradient Descent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Mini-batch Gradient Descent\n",
        "\n",
        "**Mini-batch Gradient Descent** merupakan kompromi antara Batch Gradient Descent dan Stochastic Gradient Descent.\n",
        "\n",
        "Pada pendekatan ini, gradien dihitung menggunakan **subset kecil (mini-batch)** dari data training pada setiap iterasi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.1 Keunggulan Mini-batch Gradient Descent\n",
        "\n",
        "Mini-batch Gradient Descent memiliki beberapa keunggulan:\n",
        "- Lebih stabil dibandingkan SGD\n",
        "- Lebih cepat dibandingkan Batch Gradient Descent\n",
        "- Efisien pada perangkat keras modern (CPU/GPU)\n",
        "\n",
        "Karena alasan inilah, mini-batch Gradient Descent menjadi pendekatan standar dalam training neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Secara konseptual, perbedaan antara Batch, Stochastic, dan Mini-batch Gradient Descent sering divisualisasikan untuk menunjukkan perbedaan jalur optimisasi menuju minimum fungsi biaya, sebagaimana digambarkan pada ilustrasi di buku."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Polynomial Regression\n",
        "\n",
        "Linear Regression memiliki keterbatasan ketika hubungan antara fitur dan target bersifat non-linear. Untuk mengatasi hal ini, kita dapat menggunakan **Polynomial Regression**, yaitu dengan menambahkan fitur-fitur polinomial dari fitur asli.\n",
        "\n",
        "Meskipun namanya Polynomial Regression, model ini tetap merupakan model linear terhadap parameter, karena hubungan linear terjadi pada koefisien \\( \\theta \\), bukan pada fitur."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.1 Generating Nonlinear Data\n",
        "\n",
        "Untuk mendemonstrasikan Polynomial Regression, kita membuat dataset sintetis dengan hubungan non-linear antara fitur dan target."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "m = 100\n",
        "X = 6 * np.random.rand(m, 1) - 3\n",
        "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data di atas mengikuti hubungan kuadratik dengan tambahan noise, sehingga tidak dapat dimodelkan dengan baik menggunakan Linear Regression biasa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.2 Polynomial Features\n",
        "\n",
        "Scikit-Learn menyediakan transformer `PolynomialFeatures` untuk menambahkan fitur polinomial secara otomatis.\n",
        "\n",
        "Pada contoh ini, kita menambahkan fitur hingga derajat 2 (kuadratik)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_poly = poly_features.fit_transform(X)\n",
        "X_poly[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Setiap instance kini memiliki fitur tambahan berupa kuadrat dari fitur asli, memungkinkan model menangkap hubungan non-linear."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.3 Polynomial Regression Model\n",
        "\n",
        "Setelah fitur polinomial dibuat, kita dapat melatih model Linear Regression seperti biasa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_poly, y)\n",
        "lin_reg.coef_, lin_reg.intercept_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model ini kini mampu memodelkan hubungan non-linear pada data, karena fitur-fitur polinomial telah disertakan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Underfitting and Overfitting\n",
        "\n",
        "**Underfitting** terjadi ketika model terlalu sederhana untuk menangkap pola dalam data, sedangkan **overfitting** terjadi ketika model terlalu kompleks dan mulai mempelajari noise.\n",
        "\n",
        "Polynomial Regression dengan derajat rendah cenderung underfitting, sementara derajat tinggi cenderung overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Learning Curves\n",
        "\n",
        "Untuk menganalisis apakah model mengalami underfitting atau overfitting, kita dapat menggunakan **learning curves**.\n",
        "\n",
        "Learning curve memplot performa model pada training set dan validation set sebagai fungsi dari jumlah data training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import learning_curve\n",
        "\n",
        "train_sizes, train_scores, val_scores = learning_curve(\n",
        "    LinearRegression(), X, y,\n",
        "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
        "    cv=5,\n",
        "    scoring=\"neg_mean_squared_error\"\n",
        ")\n",
        "\n",
        "train_rmse = np.sqrt(-train_scores.mean(axis=1))\n",
        "val_rmse = np.sqrt(-val_scores.mean(axis=1))\n",
        "\n",
        "train_rmse, val_rmse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Learning curves membantu kita mengidentifikasi apakah menambah data training atau meningkatkan kompleksitas model dapat memperbaiki performa.\n",
        "\n",
        "Jika kedua kurva mendekat namun error masih tinggi, model cenderung underfitting. Jika terdapat jarak besar antara kurva training dan validation, model cenderung overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Regularized Linear Models\n",
        "\n",
        "Ketika model terlalu kompleks, risiko **overfitting** meningkat. Salah satu teknik utama untuk mengatasi masalah ini adalah **regularization**, yaitu menambahkan penalti pada besar parameter model.\n",
        "\n",
        "Regularization mendorong model untuk memilih parameter yang lebih kecil, sehingga model menjadi lebih sederhana dan lebih mampu melakukan generalisasi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 11.1 Ridge Regression (L2 Regularization)\n",
        "\n",
        "Ridge Regression menambahkan penalti berupa kuadrat dari parameter ke dalam fungsi biaya. Pendekatan ini mengecilkan nilai parameter tanpa menghilangkannya sepenuhnya.\n",
        "\n",
        "Ridge Regression sangat efektif ketika semua fitur memiliki kontribusi kecil terhadap prediksi."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "ridge_reg = Ridge(alpha=1)\n",
        "ridge_reg.fit(X, y)\n",
        "ridge_reg.coef_, ridge_reg.intercept_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Parameter `alpha` mengontrol kekuatan regularization. Semakin besar nilai alpha, semakin kuat penalti yang diberikan pada parameter model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 11.2 Lasso Regression (L1 Regularization)\n",
        "\n",
        "Lasso Regression menggunakan penalti absolut dari parameter. Salah satu sifat unik Lasso adalah kemampuannya untuk membuat beberapa parameter bernilai nol, sehingga berperan sebagai **feature selection**.\n",
        "\n",
        "Pendekatan ini sangat berguna ketika hanya sebagian kecil fitur yang benar-benar relevan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "lasso_reg = Lasso(alpha=0.1)\n",
        "lasso_reg.fit(X, y)\n",
        "lasso_reg.coef_, lasso_reg.intercept_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Koefisien bernilai nol menunjukkan fitur yang dianggap tidak penting oleh model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 11.3 Elastic Net\n",
        "\n",
        "Elastic Net menggabungkan regularization L1 dan L2. Pendekatan ini berguna ketika terdapat banyak fitur yang saling berkorelasi.\n",
        "\n",
        "Elastic Net menghindari kelemahan Lasso yang cenderung memilih satu fitur dari kelompok fitur yang berkorelasi."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import ElasticNet\n",
        "\n",
        "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
        "elastic_net.fit(X, y)\n",
        "elastic_net.coef_, elastic_net.intercept_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Logistic Regression\n",
        "\n",
        "Meskipun namanya mengandung kata *regression*, **Logistic Regression** digunakan untuk **classification**, terutama binary classification.\n",
        "\n",
        "Model ini memprediksi probabilitas suatu instance termasuk ke dalam kelas tertentu menggunakan fungsi sigmoid."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 12.1 Logistic Function\n",
        "\n",
        "Fungsi sigmoid memetakan nilai input ke rentang 0 hingga 1, sehingga cocok untuk merepresentasikan probabilitas.\n",
        "\n",
        "Berdasarkan probabilitas ini, model mengambil keputusan kelas menggunakan threshold tertentu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Softmax Regression\n",
        "\n",
        "Softmax Regression merupakan generalisasi Logistic Regression untuk **multiclass classification**.\n",
        "\n",
        "Alih-alih memprediksi probabilitas satu kelas, Softmax memprediksi probabilitas untuk setiap kelas, dan memilih kelas dengan probabilitas tertinggi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Closing Summary (Chapter 4)\n",
        "\n",
        "Chapter 4 membahas secara mendalam bagaimana model Machine Learning dilatih, mulai dari pendekatan matematis hingga algoritma optimisasi yang digunakan dalam praktik.\n",
        "\n",
        "Melalui Linear Regression, Gradient Descent, Polynomial Regression, regularization, dan Logistic Regression, kita memahami bahwa proses training adalah inti dari Machine Learning.\n",
        "\n",
        "Pemahaman konsep-konsep ini menjadi fondasi penting untuk mempelajari model yang lebih kompleks seperti Support Vector Machines dan Neural Networks pada chapter berikutnya."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
