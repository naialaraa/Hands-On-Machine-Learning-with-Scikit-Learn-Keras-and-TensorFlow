{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 18: Reinforcement Learning\n",
        "\n",
        "Reinforcement Learning (RL) adalah bidang Machine Learning di mana sebuah **Agen** belajar berinteraksi dalam sebuah **Lingkungan** (*Environment*). Agen melakukan **Aksi** (*Action*) dan menerima **Imbalan** (*Reward*) atau penalti. Tujuannya adalah untuk mempelajari **Kebijakan** (*Policy*) optimal yang memaksimalkan akumulasi imbalan seiring waktu.\n",
        "\n",
        "### Konsep Utama:\n",
        "1. **Agen & Lingkungan**: Entitas yang belajar dan dunia tempat ia beraksi.\n",
        "2. **Aksi, State, & Reward**: Apa yang dilakukan agen, kondisi saat ini, dan umpan balik yang diterima.\n",
        "3. **Policy (Kebijakan)**: Algoritma yang digunakan agen untuk menentukan aksi berdasarkan state.\n",
        "4. **OpenAI Gym**: Toolkit standar untuk mengembangkan dan membandingkan algoritma RL."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Menyiapkan Lingkungan dengan OpenAI Gym\n",
        "\n",
        "Kita akan menggunakan masalah klasik **CartPole**, di mana agen harus menjaga tiang tetap tegak dengan menggerakkan kereta ke kiri atau ke kanan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# Membuat lingkungan CartPole\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "# Inisialisasi lingkungan\n",
        "obs = env.reset()\n",
        "print(\"Observasi awal (State):\", obs)\n",
        "\n",
        "# Contoh melakukan satu langkah aksi secara acak\n",
        "action = 1 # 0 untuk kiri, 1 untuk kanan\n",
        "obs, reward, done, info = env.step(action)\n",
        "\n",
        "print(f\"Observasi Baru: {obs}\")\n",
        "print(f\"Reward: {reward}\")\n",
        "print(f\"Selesai (Done): {done}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Neural Network Policies\n",
        "\n",
        "Alih-alih menggunakan aturan keras (*hard-coded*), kita bisa menggunakan Neural Network yang menerima observasi sebagai input dan mengeluarkan probabilitas untuk setiap aksi."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "n_inputs = 4 # Observasi CartPole memiliki 4 fitur\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(5, activation=\"relu\", input_shape=[n_inputs]),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\") # Output probabilitas untuk aksi '1'\n",
        "])\n",
        "\n",
        "print(\"Model Kebijakan (Policy Model) berhasil dibangun.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Algoritma Policy Gradients (REINFORCE)\n",
        "\n",
        "Policy Gradients melatih agen dengan mengevaluasi hasil dari seluruh episode. Aksi yang menghasilkan total reward tinggi akan diperkuat (probabilitasnya ditingkatkan), sedangkan aksi yang buruk akan diperlemah.\n",
        "\n",
        "### Langkah Pelatihan:\n",
        "1. Biarkan agen bermain beberapa episode.\n",
        "2. Hitung total reward untuk setiap langkah (dengan *discount factor*).\n",
        "3. Lakukan Backpropagation untuk menyesuaikan bobot model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def discount_rewards(rewards, discount_factor):\n",
        "    # Menghitung imbalan masa depan yang didiskon\n",
        "    discounted = np.array(rewards)\n",
        "    for step in range(len(rewards) - 2, -1, -1):\n",
        "        discounted[step] += discounted[step + 1] * discount_factor\n",
        "    return discounted\n",
        "\n",
        "def discount_and_normalize_rewards(all_rewards, discount_factor):\n",
        "    all_discounted_rewards = [discount_rewards(r, discount_factor) for r in all_rewards]\n",
        "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
        "    reward_mean = flat_rewards.mean()\n",
        "    reward_std = flat_rewards.std()\n",
        "    return [(r - reward_mean) / reward_std for r in all_discounted_rewards]\n",
        "\n",
        "print(\"Fungsi pemrosesan reward siap digunakan.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Q-Learning & Markov Decision Processes (MDP)\n",
        "\n",
        "Q-Learning adalah algoritma berbasis nilai yang mencoba mempelajari **Q-Value** (kualitas) dari setiap pasangan (State, Action).\n",
        "\n",
        "### Persamaan Bellman:\n",
        "$$Q(s, a) \\leftarrow (1 - \\alpha)Q(s, a) + \\alpha(r + \\gamma \\max_{a'} Q(s', a'))$$\n",
        "\n",
        "Di mana:\n",
        "- $\\alpha$ adalah *learning rate*.\n",
        "- $\\gamma$ adalah *discount factor*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Deep Q-Learning (DQN)\n",
        "\n",
        "Untuk lingkungan dengan state space yang besar, kita tidak bisa menggunakan tabel. Sebagai gantinya, kita menggunakan Neural Network untuk mengestimasi Q-Values.\n",
        "\n",
        "### Teknik Kritis dalam DQN:\n",
        "1. **Experience Replay**: Menyimpan pengalaman lama dan mengambil sampel acak untuk pelatihan (mengurangi korelasi data).\n",
        "2. **Target Network**: Menggunakan jaringan kedua yang jarang diperbarui untuk menstabilkan target prediksi Q-Value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "\n",
        "replay_buffer = deque(maxlen=2000)\n",
        "\n",
        "def epsilon_greedy_policy(state, epsilon=0):\n",
        "    # Eksplorasi vs Eksploitasi\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.randint(2)\n",
        "    else:\n",
        "        Q_values = model.predict(state[np.newaxis])\n",
        "        return np.argmax(Q_values[0])\n",
        "\n",
        "print(\"Komponen dasar Deep Q-Learning siap.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Kesimpulan Praktis\n",
        "\n",
        "1. **Reinforcement Learning** sangat berbeda dari supervised learning karena target (imbalan) seringkali tertunda (*delayed rewards*).\n",
        "2. **Policy Gradients** efektif untuk aksi kontinu dan ruang observasi besar.\n",
        "3. **DQN** adalah pondasi untuk banyak kesuksesan RL modern (seperti agen Atari), tetapi memerlukan teknik seperti *Experience Replay* untuk stabilitas.\n",
        "4. **TF-Agents** adalah library TensorFlow yang sangat direkomendasikan untuk implementasi RL tingkat produksi di dunia nyata."
      ]
    }

 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5

}
