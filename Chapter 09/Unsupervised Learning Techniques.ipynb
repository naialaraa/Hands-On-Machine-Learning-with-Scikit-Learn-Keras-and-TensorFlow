{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 9: Unsupervised Learning Techniques\n",
        "\n",
        "Pada chapter ini, kita akan mempelajari **unsupervised learning**, yaitu pendekatan Machine Learning yang bekerja dengan **data tanpa label**.\n",
        "\n",
        "Berbeda dengan supervised learning yang menggunakan pasangan data `(X, y)`, pada unsupervised learning kita hanya memiliki data input `X` tanpa informasi target yang jelas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Pengantar Unsupervised Learning\n",
        "\n",
        "Sebagian besar data di dunia nyata sebenarnya **tidak berlabel**.\n",
        "\n",
        "Contohnya:\n",
        "- data perilaku pengguna website,\n",
        "- data transaksi pelanggan,\n",
        "- data sensor industri,\n",
        "- data gambar atau suara dalam jumlah besar.\n",
        "\n",
        "Memberi label pada data-data tersebut sering kali mahal, memakan waktu, dan membutuhkan keahlian khusus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unsupervised learning bertujuan untuk **menemukan struktur tersembunyi** di dalam data tanpa bantuan label.\n",
        "\n",
        "Pendekatan ini sangat penting karena memungkinkan model belajar langsung dari pola alami data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Yann LeCun pernah menyatakan bahwa:\n",
        "\n",
        "> *Jika kecerdasan adalah sebuah kue, maka unsupervised learning adalah kuenya, supervised learning adalah icing-nya, dan reinforcement learning adalah ceri di atasnya.*\n",
        "\n",
        "Pernyataan ini menekankan bahwa unsupervised learning merupakan fondasi utama dalam pengembangan kecerdasan buatan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Tugas Utama dalam Unsupervised Learning\n",
        "\n",
        "Pada chapter ini, kita akan membahas beberapa tugas utama dalam unsupervised learning, yaitu:\n",
        "\n",
        "- **Clustering**: mengelompokkan data berdasarkan kemiripan,\n",
        "- **Anomaly Detection**: mendeteksi data yang menyimpang dari pola normal,\n",
        "- **Density Estimation**: memperkirakan distribusi probabilitas data.\n",
        "\n",
        "Masing-masing tugas ini memiliki aplikasi nyata yang sangat luas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Hubungan dengan Chapter Sebelumnya\n",
        "\n",
        "Pada **Chapter 8**, kita telah mempelajari dimensionality reduction, yang merupakan salah satu bentuk unsupervised learning.\n",
        "\n",
        "Chapter 9 memperluas konsep tersebut dengan membahas teknik unsupervised learning lainnya yang lebih berfokus pada:\n",
        "- pengelompokan data,\n",
        "- pemahaman struktur data,\n",
        "- serta deteksi pola tidak normal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Clustering\n",
        "\n",
        "**Clustering** adalah tugas utama dalam unsupervised learning yang bertujuan untuk **mengelompokkan data** ke dalam beberapa grup (cluster) berdasarkan tingkat kemiripan.\n",
        "\n",
        "Tidak seperti klasifikasi, pada clustering:\n",
        "- tidak ada label awal,\n",
        "- jumlah dan bentuk cluster tidak selalu diketahui,\n",
        "- evaluasi hasil sering kali bersifat subjektif atau kontekstual."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Intuisi Clustering\n",
        "\n",
        "Intuisi dasar clustering adalah:\n",
        "- data dalam satu cluster **lebih mirip satu sama lain**,\n",
        "- data dari cluster yang berbeda **lebih tidak mirip**.\n",
        "\n",
        "Kemiripan biasanya diukur menggunakan metrik jarak, seperti **Euclidean distance**, tetapi dapat juga menggunakan metrik lain tergantung konteks data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sebagai ilustrasi:\n",
        "- pelanggan dengan pola belanja yang mirip dikelompokkan bersama,\n",
        "- dokumen dengan topik serupa masuk ke cluster yang sama,\n",
        "- titik data yang saling berdekatan di ruang fitur dianggap satu kelompok."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Use Cases Clustering\n",
        "\n",
        "Clustering digunakan secara luas dalam berbagai aplikasi, antara lain:\n",
        "- **Customer segmentation** dalam bisnis dan pemasaran,\n",
        "- **Document clustering** untuk pengelompokan berita atau artikel,\n",
        "- **Image segmentation** dalam pengolahan citra,\n",
        "- **Bioinformatics** untuk analisis ekspresi gen,\n",
        "- **Anomaly detection** sebagai tahap awal eksplorasi data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dalam banyak kasus, clustering bukanlah tujuan akhir, melainkan:\n",
        "- alat eksplorasi data,\n",
        "- fitur tambahan untuk model supervised,\n",
        "- atau dasar pengambilan keputusan lebih lanjut."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Tantangan dalam Clustering\n",
        "\n",
        "Beberapa tantangan utama dalam clustering meliputi:\n",
        "- menentukan jumlah cluster yang tepat,\n",
        "- memilih metrik jarak yang sesuai,\n",
        "- menangani data berdimensi tinggi,\n",
        "- mengevaluasi kualitas cluster tanpa label.\n",
        "\n",
        "Karena itu, tidak ada satu algoritma clustering yang cocok untuk semua kasus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. K-Means Clustering\n",
        "\n",
        "**K-Means** merupakan algoritma clustering yang paling populer dan paling sederhana.\n",
        "\n",
        "Tujuan K-Means adalah membagi data ke dalam **K cluster** sedemikian rupa sehingga jarak antara setiap titik data dengan pusat cluster-nya (centroid) menjadi sekecil mungkin."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 Intuisi Dasar K-Means\n",
        "\n",
        "Secara intuitif, K-Means bekerja dengan cara:\n",
        "1. memilih K titik awal sebagai centroid,\n",
        "2. menetapkan setiap data ke centroid terdekat,\n",
        "3. menghitung ulang posisi centroid sebagai rata-rata titik dalam cluster,\n",
        "4. mengulangi langkah 2 dan 3 hingga konvergen.\n",
        "\n",
        "Proses ini bertujuan untuk meminimalkan **within-cluster variance**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Fungsi Objektif K-Means\n",
        "\n",
        "Secara matematis, K-Means meminimalkan fungsi objektif berikut:\n",
        "\n",
        "\\n\\n\\( J = \\sum_{i=1}^{K} \\sum_{x \\in C_i} ||x - \\mu_i||^2 \\)\n",
        "\n",
        "di mana:\n",
        "- \\(C_i\\) adalah cluster ke-i,\n",
        "- \\(\\mu_i\\) adalah centroid cluster ke-i.\n",
        "\n",
        "Fungsi ini dikenal sebagai **inertia** atau **distortion**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3 K-Means dengan Scikit-Learn\n",
        "\n",
        "Scikit-Learn menyediakan implementasi K-Means melalui class `KMeans`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "X_blobs, y_blobs = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
        "\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "kmeans.fit(X_blobs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Setelah training, kita dapat mengakses posisi centroid dan label cluster hasil K-Means."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "kmeans.cluster_centers_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "kmeans.labels_[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hasil di atas menunjukkan bahwa setiap data telah ditetapkan ke salah satu dari 4 cluster.\n",
        "\n",
        "K-Means bekerja sangat baik pada data dengan cluster berbentuk bulat dan ukuran yang relatif seragam."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Evaluating K-Means: Inertia\n",
        "\n",
        "Untuk mengevaluasi hasil K-Means, metrik yang paling umum digunakan adalah **inertia**.\n",
        "\n",
        "Inertia mengukur seberapa dekat setiap titik data dengan centroid cluster-nya masing-masing.\n",
        "\n",
        "Semakin kecil nilai inertia, semakin kompak cluster yang dihasilkan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Namun, nilai inertia **selalu menurun** ketika jumlah cluster (K) ditambah.\n",
        "\n",
        "Oleh karena itu, inertia tidak dapat digunakan sendiri untuk menentukan jumlah cluster yang optimal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 The Elbow Method\n",
        "\n",
        "Salah satu pendekatan populer untuk memilih jumlah cluster adalah **Elbow Method**.\n",
        "\n",
        "Metode ini mencari titik di mana penurunan inertia mulai melambat secara signifikan, menyerupai bentuk siku (*elbow*)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inertias = []\n",
        "for k in range(1, 10):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(X_blobs)\n",
        "    inertias.append(kmeans.inertia_)\n",
        "\n",
        "inertias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dengan memplot nilai inertia terhadap jumlah cluster, kita dapat mengamati titik elbow secara visual.\n",
        "\n",
        "Namun, dalam praktik, elbow tidak selalu jelas terlihat."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Initialization of Centroids\n",
        "\n",
        "K-Means sangat sensitif terhadap pemilihan centroid awal.\n",
        "\n",
        "Inisialisasi yang buruk dapat menyebabkan:\n",
        "- konvergensi ke solusi lokal yang buruk,\n",
        "- waktu training yang lebih lama."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.1 K-Means++\n",
        "\n",
        "**K-Means++** merupakan metode inisialisasi centroid yang dirancang untuk mengatasi masalah ini.\n",
        "\n",
        "Metode ini memilih centroid awal secara bertahap dengan memastikan centroid tersebar sejauh mungkin satu sama lain.\n",
        "\n",
        "Secara default, Scikit-Learn menggunakan K-Means++."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Limitations of K-Means\n",
        "\n",
        "Meskipun populer, K-Means memiliki beberapa keterbatasan penting:\n",
        "- harus menentukan jumlah cluster di awal,\n",
        "- sensitif terhadap outlier,\n",
        "- hanya efektif untuk cluster berbentuk bulat dan berukuran serupa,\n",
        "- tidak cocok untuk cluster dengan densitas berbeda.\n",
        "\n",
        "Karena keterbatasan ini, algoritma clustering lain sering kali lebih sesuai untuk data kompleks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. K-Means for Image Segmentation\n",
        "\n",
        "Selain untuk clustering data numerik, **K-Means** juga sering digunakan untuk **image segmentation**, yaitu proses membagi gambar menjadi beberapa region berdasarkan kemiripan warna atau fitur piksel.\n",
        "\n",
        "Pada image segmentation, setiap piksel diperlakukan sebagai sebuah data point, dan fitur yang digunakan biasanya adalah nilai warna (misalnya RGB)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.1 Intuisi Image Segmentation dengan K-Means\n",
        "\n",
        "Intuisi dasarnya adalah:\n",
        "- piksel dengan warna yang mirip akan dikelompokkan ke dalam cluster yang sama,\n",
        "- setiap cluster merepresentasikan satu region warna dominan,\n",
        "- gambar hasil segmentasi akan memiliki jumlah warna yang lebih sedikit.\n",
        "\n",
        "Pendekatan ini sering digunakan untuk kompresi gambar dan analisis visual sederhana."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.2 Contoh Implementasi Konseptual\n",
        "\n",
        "Sebagai ilustrasi sederhana, misalkan kita memiliki sebuah gambar dengan banyak warna.\n",
        "\n",
        "Dengan menerapkan K-Means pada nilai RGB piksel, kita dapat mengelompokkan piksel-piksel tersebut ke dalam K cluster warna."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# contoh data piksel (RGB) sederhana\n",
        "pixels = np.array([\n",
        "    [255, 0, 0],\n",
        "    [254, 0, 0],\n",
        "    [0, 255, 0],\n",
        "    [0, 254, 0],\n",
        "    [0, 0, 255],\n",
        "    [0, 0, 254]\n",
        "])\n",
        "\n",
        "kmeans_img = KMeans(n_clusters=3, random_state=42)\n",
        "kmeans_img.fit(pixels)\n",
        "\n",
        "kmeans_img.cluster_centers_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Centroid hasil clustering merepresentasikan **warna dominan** dari setiap cluster.\n",
        "\n",
        "Setiap piksel kemudian dapat digantikan dengan warna centroid cluster-nya masing-masing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.3 Kelebihan dan Kekurangan\n",
        "\n",
        "Kelebihan K-Means untuk image segmentation:\n",
        "- sederhana dan cepat,\n",
        "- mudah diimplementasikan,\n",
        "- efektif untuk gambar dengan warna yang relatif jelas.\n",
        "\n",
        "Namun, pendekatan ini memiliki keterbatasan:\n",
        "- tidak mempertimbangkan posisi spasial piksel,\n",
        "- sensitif terhadap noise,\n",
        "- hasil sangat bergantung pada jumlah cluster K."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Gaussian Mixture Models (GMM)\n",
        "\n",
        "Selain K-Means, pendekatan populer lain untuk clustering adalah **Gaussian Mixture Models (GMM)**.\n",
        "\n",
        "GMM mengasumsikan bahwa data dihasilkan dari campuran beberapa distribusi Gaussian dengan parameter yang berbeda."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10.1 Intuisi Gaussian Mixture Models\n",
        "\n",
        "Berbeda dengan K-Means yang menetapkan setiap data ke satu cluster secara tegas (*hard assignment*),\n",
        "GMM melakukan **soft assignment**, yaitu setiap data memiliki probabilitas untuk berada di setiap cluster.\n",
        "\n",
        "Pendekatan ini lebih fleksibel dan mampu menangkap cluster dengan bentuk elips dan ukuran yang berbeda."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Secara konseptual, GMM memodelkan data sebagai:\n",
        "- kombinasi beberapa distribusi Gaussian,\n",
        "- masing-masing Gaussian merepresentasikan satu cluster,\n",
        "- parameter yang dipelajari meliputi mean, covariance, dan weight."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10.2 Expectation-Maximization (EM) Algorithm\n",
        "\n",
        "GMM dilatih menggunakan algoritma **Expectation-Maximization (EM)**.\n",
        "\n",
        "Algoritma EM bekerja secara iteratif melalui dua langkah:\n",
        "- **Expectation (E-step)**: menghitung probabilitas setiap data berasal dari masing-masing Gaussian,\n",
        "- **Maximization (M-step)**: memperbarui parameter Gaussian untuk memaksimalkan likelihood.\n",
        "\n",
        "Proses ini diulang hingga konvergen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10.3 GMM dengan Scikit-Learn\n",
        "\n",
        "Scikit-Learn menyediakan implementasi GMM melalui class `GaussianMixture`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "gmm = GaussianMixture(n_components=4, random_state=42)\n",
        "gmm.fit(X_blobs)\n",
        "\n",
        "gmm.means_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mean dari masing-masing Gaussian merepresentasikan pusat cluster hasil GMM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gmm.predict(X_blobs[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prediksi GMM memberikan label cluster berdasarkan Gaussian dengan probabilitas tertinggi.\n",
        "\n",
        "Namun, kita juga dapat mengakses probabilitas keanggotaan setiap cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gmm.predict_proba(X_blobs[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Soft assignment ini membuat GMM lebih ekspresif dibandingkan K-Means, terutama ketika cluster saling tumpang tindih."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Selecting the Number of Clusters\n",
        "\n",
        "Menentukan jumlah cluster yang tepat merupakan salah satu tantangan utama dalam unsupervised learning.\n",
        "\n",
        "Pada Gaussian Mixture Models (GMM), pemilihan jumlah cluster dapat dilakukan secara lebih sistematis menggunakan **kriteria informasi**, seperti **AIC** dan **BIC**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 11.1 Akaike Information Criterion (AIC)\n",
        "\n",
        "**Akaike Information Criterion (AIC)** mengukur kualitas model dengan mempertimbangkan dua hal:\n",
        "- seberapa baik model menjelaskan data (likelihood),\n",
        "- kompleksitas model (jumlah parameter).\n",
        "\n",
        "Model dengan nilai AIC **lebih kecil** dianggap lebih baik."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 11.2 Bayesian Information Criterion (BIC)\n",
        "\n",
        "**Bayesian Information Criterion (BIC)** mirip dengan AIC, tetapi memberikan penalti yang **lebih kuat** terhadap kompleksitas model.\n",
        "\n",
        "Akibatnya, BIC cenderung memilih model yang lebih sederhana dibandingkan AIC."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Secara praktik:\n",
        "- AIC lebih toleran terhadap model kompleks,\n",
        "- BIC lebih konservatif dan sering menghasilkan jumlah cluster yang lebih kecil."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 11.3 Menggunakan AIC dan BIC pada GMM\n",
        "\n",
        "Scikit-Learn menyediakan metode `aic()` dan `bic()` untuk mengevaluasi model GMM dengan jumlah komponen yang berbeda."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "aics = []\n",
        "bics = []\n",
        "\n",
        "for k in range(1, 10):\n",
        "    gmm = GaussianMixture(n_components=k, random_state=42)\n",
        "    gmm.fit(X_blobs)\n",
        "    aics.append(gmm.aic(X_blobs))\n",
        "    bics.append(gmm.bic(X_blobs))\n",
        "\n",
        "aics, bics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dengan memplot nilai AIC dan BIC terhadap jumlah komponen, kita dapat memilih jumlah cluster yang meminimalkan kriteria tersebut.\n",
        "\n",
        "Pendekatan ini lebih objektif dibandingkan metode visual seperti elbow method pada K-Means."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Anomaly Detection\n",
        "\n",
        "**Anomaly detection** bertujuan untuk mengidentifikasi data yang menyimpang secara signifikan dari pola umum.\n",
        "\n",
        "Data anomali sering kali merepresentasikan kejadian penting, seperti:\n",
        "- transaksi penipuan,\n",
        "- kegagalan sistem,\n",
        "- aktivitas mencurigakan,\n",
        "- atau kesalahan pengukuran."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 12.1 Tantangan dalam Anomaly Detection\n",
        "\n",
        "Beberapa tantangan utama dalam anomaly detection antara lain:\n",
        "- anomali jarang terjadi,\n",
        "- definisi anomali sering bergantung pada konteks,\n",
        "- data biasanya tidak berlabel.\n",
        "\n",
        "Karena itu, banyak teknik anomaly detection bersifat unsupervised."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12.2 Isolation Forest\n",
        "\n",
        "**Isolation Forest** mendeteksi anomali dengan cara mengisolasi instance data.\n",
        "\n",
        "Intuisi utamanya adalah bahwa anomali:\n",
        "- lebih jarang,\n",
        "- lebih berbeda dari data normal,\n",
        "- sehingga lebih mudah diisolasi dengan sedikit split."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Isolation Forest membangun banyak pohon acak.\n",
        "\n",
        "Instance yang terisolasi lebih cepat (kedalaman pohon lebih kecil) dianggap sebagai anomali."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
        "iso_forest.fit(X_blobs)\n",
        "\n",
        "iso_forest.predict(X_blobs[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output bernilai `-1` menunjukkan data dianggap sebagai anomali, sedangkan `1` menunjukkan data normal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12.3 Local Outlier Factor (LOF)\n",
        "\n",
        "**Local Outlier Factor (LOF)** mendeteksi anomali dengan membandingkan kepadatan lokal sebuah titik data terhadap tetangganya.\n",
        "\n",
        "Jika kepadatan lokal suatu titik jauh lebih rendah dibandingkan tetangganya, titik tersebut dianggap sebagai outlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "\n",
        "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.05)\n",
        "y_pred_lof = lof.fit_predict(X_blobs)\n",
        "\n",
        "y_pred_lof[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "LOF sangat efektif untuk mendeteksi outlier yang hanya menyimpang secara lokal, tetapi kurang cocok untuk prediksi pada data baru."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Novelty Detection\n",
        "\n",
        "**Novelty detection** bertujuan untuk mendeteksi data baru yang berbeda secara signifikan dari data yang dianggap normal.\n",
        "\n",
        "Berbeda dengan anomaly detection yang dapat dilatih menggunakan data yang mengandung anomali, novelty detection **diasumsikan hanya dilatih pada data normal**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Perbedaan utama:\n",
        "- **Anomaly detection**: data training dapat mengandung outlier,\n",
        "- **Novelty detection**: data training diharapkan bersih dari outlier.\n",
        "\n",
        "Novelty detection umum digunakan dalam sistem monitoring dan quality control."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13.1 One-Class Support Vector Machine (One-Class SVM)\n",
        "\n",
        "**One-Class SVM** merupakan algoritma populer untuk novelty detection.\n",
        "\n",
        "Tujuan One-Class SVM adalah mempelajari batas (boundary) yang mencakup sebagian besar data normal dan mengklasifikasikan data di luar batas tersebut sebagai anomali."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "One-Class SVM bekerja dengan:\n",
        "- memproyeksikan data ke ruang fitur berdimensi tinggi menggunakan kernel,\n",
        "- mencari hyperplane yang memisahkan data normal dari titik asal,\n",
        "- memaksimalkan margin dari data normal.\n",
        "\n",
        "Pendekatan ini mirip dengan SVM, tetapi tanpa kelas negatif eksplisit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 13.2 One-Class SVM dengan Scikit-Learn\n",
        "\n",
        "Scikit-Learn menyediakan implementasi One-Class SVM melalui class `OneClassSVM`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.svm import OneClassSVM\n",
        "\n",
        "one_class_svm = OneClassSVM(kernel=\"rbf\", gamma=0.1, nu=0.05)\n",
        "one_class_svm.fit(X_blobs)\n",
        "\n",
        "one_class_svm.predict(X_blobs[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output bernilai `-1` menunjukkan data dianggap sebagai novelty (tidak normal), sedangkan `1` menunjukkan data normal.\n",
        "\n",
        "Parameter penting pada One-Class SVM:\n",
        "- `nu`: perkiraan proporsi outlier,\n",
        "- `gamma`: menentukan kompleksitas boundary.\n",
        "\n",
        "Pemilihan parameter yang tepat sangat krusial untuk performa model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Density Estimation\n",
        "\n",
        "**Density estimation** bertujuan untuk memperkirakan **distribusi probabilitas** yang mendasari data.\n",
        "\n",
        "Dengan mengetahui distribusi data, kita dapat:\n",
        "- mendeteksi anomali (data dengan probabilitas sangat rendah),\n",
        "- melakukan sampling data baru,\n",
        "- memahami struktur global data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14.1 Kernel Density Estimation (KDE)\n",
        "\n",
        "**Kernel Density Estimation (KDE)** merupakan metode non-parametrik untuk memperkirakan fungsi densitas probabilitas.\n",
        "\n",
        "Berbeda dengan GMM yang mengasumsikan bentuk distribusi tertentu (Gaussian), KDE tidak membuat asumsi bentuk distribusi data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Intuisi KDE:\n",
        "- setiap titik data dianggap sebagai pusat sebuah *kernel* (misalnya Gaussian),\n",
        "- densitas total diperoleh dengan menjumlahkan kontribusi kernel dari seluruh titik data.\n",
        "\n",
        "Hasilnya adalah estimasi distribusi yang halus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 14.2 Parameter Bandwidth\n",
        "\n",
        "Parameter terpenting dalam KDE adalah **bandwidth**.\n",
        "\n",
        "- bandwidth kecil → distribusi sangat detail (risiko overfitting),\n",
        "- bandwidth besar → distribusi terlalu halus (risiko underfitting).\n",
        "\n",
        "Pemilihan bandwidth yang tepat sangat krusial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 14.3 KDE dengan Scikit-Learn\n",
        "\n",
        "Scikit-Learn menyediakan implementasi KDE melalui class `KernelDensity`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KernelDensity\n",
        "\n",
        "kde = KernelDensity(kernel=\"gaussian\", bandwidth=0.5)\n",
        "kde.fit(X_blobs)\n",
        "\n",
        "log_density = kde.score_samples(X_blobs[:10])\n",
        "log_density"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Metode `score_samples()` mengembalikan **log-density** dari setiap data point.\n",
        "\n",
        "Nilai log-density yang sangat kecil (negatif besar) dapat mengindikasikan anomali."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. Semi-Supervised Learning\n",
        "\n",
        "**Semi-supervised learning** berada di antara supervised dan unsupervised learning.\n",
        "\n",
        "Pendekatan ini digunakan ketika:\n",
        "- sebagian kecil data memiliki label,\n",
        "- sebagian besar data tidak berlabel.\n",
        "\n",
        "Kondisi ini sangat umum di dunia nyata karena proses pelabelan data sering kali mahal dan memakan waktu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 15.1 Intuisi Semi-Supervised Learning\n",
        "\n",
        "Ide utama semi-supervised learning adalah memanfaatkan **struktur alami data tidak berlabel** untuk membantu proses pembelajaran.\n",
        "\n",
        "Dengan asumsi bahwa data dengan struktur atau kedekatan yang mirip cenderung memiliki label yang sama, model dapat memperluas informasi dari data berlabel ke data tidak berlabel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pendekatan ini sering kali memberikan performa yang jauh lebih baik dibandingkan:\n",
        "- supervised learning dengan data berlabel yang sangat sedikit,\n",
        "- unsupervised learning murni tanpa memanfaatkan label sama sekali."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15.2 Label Propagation\n",
        "\n",
        "**Label Propagation** adalah salah satu algoritma semi-supervised learning yang populer.\n",
        "\n",
        "Algoritma ini menyebarkan (*propagate*) label dari data berlabel ke data tidak berlabel berdasarkan kemiripan antar data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Intuisi Label Propagation:\n",
        "- data yang saling berdekatan kemungkinan besar memiliki label yang sama,\n",
        "- label menyebar melalui graf kemiripan data,\n",
        "- proses diulang hingga label stabil.\n",
        "\n",
        "Pendekatan ini sangat efektif ketika data membentuk cluster yang jelas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 15.3 Label Propagation dengan Scikit-Learn\n",
        "\n",
        "Scikit-Learn menyediakan implementasi Label Propagation melalui class `LabelPropagation` dan `LabelSpreading`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.semi_supervised import LabelPropagation\n",
        "\n",
        "# contoh sederhana: sebagian label dihilangkan\n",
        "y_semi = np.copy(y_blobs)\n",
        "rng = np.random.RandomState(42)\n",
        "unlabeled_indices = rng.choice(len(y_semi), size=200, replace=False)\n",
        "y_semi[unlabeled_indices] = -1\n",
        "\n",
        "label_prop = LabelPropagation()\n",
        "label_prop.fit(X_blobs, y_semi)\n",
        "\n",
        "label_prop.transduction_[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hasil `transduction_` menunjukkan label hasil propagasi untuk seluruh data.\n",
        "\n",
        "Pendekatan ini memungkinkan kita memanfaatkan data tidak berlabel secara efektif, terutama ketika struktur cluster data cukup jelas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Closing Summary (Chapter 9)\n",
        "\n",
        "Chapter 9 membahas berbagai **teknik unsupervised learning** yang memungkinkan model Machine Learning belajar langsung dari data tanpa label.\n",
        "\n",
        "Berbeda dengan supervised learning, fokus utama unsupervised learning adalah **menemukan struktur tersembunyi** di dalam data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Konsep dan teknik utama yang dipelajari dalam chapter ini meliputi:\n",
        "- **Clustering**, dengan algoritma seperti K-Means dan Gaussian Mixture Models (GMM),\n",
        "- metode evaluasi dan pemilihan jumlah cluster (Elbow Method, AIC, dan BIC),\n",
        "- **Anomaly Detection** menggunakan Isolation Forest dan Local Outlier Factor (LOF),\n",
        "- **Novelty Detection** dengan One-Class SVM,\n",
        "- **Density Estimation** menggunakan Kernel Density Estimation (KDE),\n",
        "- serta **Semi-Supervised Learning** melalui Label Propagation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Melalui berbagai contoh dan algoritma, chapter ini menunjukkan bahwa:\n",
        "- tidak ada satu algoritma unsupervised learning yang cocok untuk semua kasus,\n",
        "- pemilihan metode sangat bergantung pada struktur data dan tujuan analisis,\n",
        "- interpretasi hasil sering kali memerlukan pemahaman konteks data.\n",
        "\n",
        "Unsupervised learning sering digunakan sebagai tahap eksplorasi awal, preprocessing, atau pendukung sistem yang lebih kompleks."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
