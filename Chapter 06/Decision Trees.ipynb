{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 6: Decision Trees\n",
        "\n",
        "Decision Trees merupakan algoritma Machine Learning yang **sangat intuitif dan mudah dipahami**, namun tetap memiliki kemampuan untuk memodelkan hubungan yang kompleks.\n",
        "\n",
        "Decision Tree dapat digunakan untuk:\n",
        "- classification,\n",
        "- regression,\n",
        "- bahkan multi-output tasks.\n",
        "\n",
        "Pada chapter ini, kita akan mempelajari bagaimana Decision Tree bekerja, bagaimana cara melatih dan memvisualisasikannya, serta memahami kekuatan dan keterbatasannya.\n",
        "\n",
        "Decision Trees juga menjadi komponen dasar dari algoritma ensemble yang sangat populer seperti **Random Forest**, sehingga pemahaman chapter ini sangat penting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Training and Visualizing a Decision Tree\n",
        "\n",
        "Untuk memahami cara kerja Decision Tree, kita akan melatih sebuah model **DecisionTreeClassifier** menggunakan dataset Iris.\n",
        "\n",
        "Dataset ini sering digunakan sebagai contoh karena sederhana, terstruktur dengan baik, dan mudah divisualisasikan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Training a Decision Tree Classifier\n",
        "\n",
        "Pada contoh ini, kita hanya menggunakan dua fitur:\n",
        "- petal length\n",
        "- petal width\n",
        "\n",
        "Pembatasan jumlah fitur ini memudahkan kita untuk memahami bagaimana Decision Tree membuat keputusan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data[:, 2:]  # petal length and petal width\n",
        "y = iris.target\n",
        "\n",
        "tree_clf = DecisionTreeClassifier(max_depth=2)\n",
        "tree_clf.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model Decision Tree di atas dibatasi dengan `max_depth=2`, sehingga struktur pohon tetap sederhana dan mudah diinterpretasikan.\n",
        "\n",
        "Tanpa pembatasan ini, Decision Tree cenderung tumbuh sangat dalam dan berisiko **overfitting**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Visualizing the Decision Tree\n",
        "\n",
        "Salah satu keunggulan utama Decision Tree adalah kemampuannya untuk divisualisasikan.\n",
        "\n",
        "Visualisasi ini membantu kita memahami aturan keputusan yang dipelajari model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.tree import export_graphviz\n",
        "\n",
        "export_graphviz(\n",
        "    tree_clf,\n",
        "    out_file=\"iris_tree.dot\",\n",
        "    feature_names=iris.feature_names[2:],\n",
        "    class_names=iris.target_names,\n",
        "    rounded=True,\n",
        "    filled=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "File `.dot` yang dihasilkan dapat dikonversi menjadi gambar menggunakan **Graphviz**.\n",
        "\n",
        "Visualisasi pohon ini menunjukkan bagaimana Decision Tree membagi ruang fitur berdasarkan threshold tertentu, seperti yang diilustrasikan pada gambar di buku."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. How Decision Trees Make Predictions\n",
        "\n",
        "Untuk memahami Decision Tree secara mendalam, kita perlu memahami **bagaimana model ini membuat prediksi**.\n",
        "\n",
        "Decision Tree bekerja dengan cara menelusuri pohon dari **root node** hingga **leaf node**, berdasarkan aturan keputusan (*decision rules*) yang dipelajari selama training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Traversing the Tree\n",
        "\n",
        "Setiap node internal pada Decision Tree berisi sebuah pertanyaan berbentuk kondisi, misalnya:\n",
        "\n",
        "> Apakah *petal length* ≤ 2.45?\n",
        "\n",
        "Jika kondisi bernilai benar, instance akan diarahkan ke cabang kiri; jika salah, ke cabang kanan.\n",
        "\n",
        "Proses ini berulang hingga mencapai leaf node, di mana prediksi akhir dibuat."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Node Information Explained\n",
        "\n",
        "Setiap node pada visualisasi Decision Tree biasanya menampilkan beberapa informasi penting:\n",
        "\n",
        "- **gini**: ukuran impurity (ketidakmurnian) pada node\n",
        "- **samples**: jumlah instance data yang mencapai node tersebut\n",
        "- **value**: distribusi jumlah instance pada setiap kelas\n",
        "- **class**: kelas mayoritas pada node tersebut\n",
        "\n",
        "Informasi ini membantu kita memahami keputusan yang dibuat oleh model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Gini Impurity\n",
        "\n",
        "**Gini impurity** mengukur seberapa sering sebuah instance akan salah diklasifikasikan jika kita memilih kelas secara acak berdasarkan distribusi kelas pada node tersebut.\n",
        "\n",
        "Nilai Gini impurity:\n",
        "- 0 → semua instance pada node berasal dari satu kelas (node murni)\n",
        "- semakin besar → semakin bercampur kelasnya\n",
        "\n",
        "Decision Tree berusaha meminimalkan Gini impurity pada setiap split."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4 Making a Prediction (Conceptual Example)\n",
        "\n",
        "Misalkan sebuah bunga Iris memiliki:\n",
        "- petal length = 5.0\n",
        "- petal width = 1.5\n",
        "\n",
        "Model akan mengevaluasi kondisi di root node terlebih dahulu, lalu mengikuti cabang yang sesuai hingga mencapai leaf node.\n",
        "\n",
        "Kelas yang diprediksi adalah kelas mayoritas pada leaf node tersebut."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.5 Predicting Class Probabilities\n",
        "\n",
        "Selain memprediksi kelas, Decision Tree juga dapat menghasilkan **probabilitas kelas**.\n",
        "\n",
        "Probabilitas ini dihitung berdasarkan proporsi kelas pada leaf node tempat instance berakhir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tree_clf.predict_proba([[5, 1.5]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output di atas menunjukkan probabilitas setiap kelas Iris.\n",
        "\n",
        "Kelas dengan probabilitas tertinggi akan dipilih sebagai hasil prediksi akhir."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Estimating Class Probabilities\n",
        "\n",
        "Selain menghasilkan prediksi kelas, Decision Tree juga mampu memperkirakan **probabilitas setiap kelas**.\n",
        "\n",
        "Probabilitas ini dihitung berdasarkan distribusi kelas pada **leaf node** tempat sebuah instance berakhir.\n",
        "\n",
        "Pendekatan ini membuat Decision Tree cukup informatif, karena kita tidak hanya mengetahui hasil prediksi, tetapi juga tingkat keyakinan model terhadap prediksi tersebut."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Secara intuitif:\n",
        "- jika sebuah leaf node berisi instance dari satu kelas saja, probabilitas kelas tersebut adalah 100%\n",
        "- jika leaf node berisi campuran beberapa kelas, probabilitas dihitung sebagai proporsi masing-masing kelas\n",
        "\n",
        "Probabilitas ini sering digunakan dalam sistem yang membutuhkan **confidence score**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. The CART Training Algorithm\n",
        "\n",
        "Decision Tree pada Scikit-Learn menggunakan algoritma **CART (Classification and Regression Trees)**.\n",
        "\n",
        "CART bekerja dengan mencari split terbaik pada setiap node yang menghasilkan pemisahan data paling murni berdasarkan kriteria tertentu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Gini Impurity\n",
        "\n",
        "Pada classification, kriteria default yang digunakan oleh CART adalah **Gini impurity**.\n",
        "\n",
        "Gini impurity mengukur seberapa sering sebuah instance akan salah diklasifikasikan jika kita secara acak memberikan label berdasarkan distribusi kelas pada node tersebut.\n",
        "\n",
        "Decision Tree memilih split yang menghasilkan **penurunan Gini impurity terbesar**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Entropy (Information Gain)\n",
        "\n",
        "Sebagai alternatif dari Gini impurity, Decision Tree juga dapat menggunakan **entropy** sebagai kriteria split.\n",
        "\n",
        "Entropy berasal dari teori informasi dan mengukur tingkat ketidakpastian pada sebuah node.\n",
        "\n",
        "Split terbaik adalah split yang memberikan **information gain** terbesar, yaitu penurunan entropy yang paling signifikan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Gini vs Entropy\n",
        "\n",
        "Secara praktik, perbedaan antara Gini dan entropy biasanya kecil.\n",
        "\n",
        "- **Gini impurity** cenderung sedikit lebih cepat secara komputasi\n",
        "- **Entropy** cenderung menghasilkan pohon yang sedikit lebih seimbang\n",
        "\n",
        "Pada kebanyakan kasus, penggunaan Gini impurity sudah cukup dan menjadi pilihan default."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.4 Using Entropy in Scikit-Learn\n",
        "\n",
        "Scikit-Learn memungkinkan kita untuk memilih kriteria split dengan mengatur parameter `criterion`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tree_clf_entropy = DecisionTreeClassifier(criterion=\"entropy\", max_depth=2)\n",
        "tree_clf_entropy.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model di atas menggunakan entropy sebagai kriteria split.\n",
        "\n",
        "Struktur pohon yang dihasilkan mungkin sedikit berbeda, tetapi performa umumnya mirip dengan model berbasis Gini."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Regularization Hyperparameters\n",
        "\n",
        "Salah satu kelemahan utama Decision Tree adalah kecenderungannya untuk **overfitting**, terutama jika pohon dibiarkan tumbuh tanpa batas.\n",
        "\n",
        "Untuk mengatasi hal ini, Decision Tree menyediakan berbagai **hyperparameter regularization** yang berfungsi membatasi kompleksitas model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 Maximum Depth (`max_depth`)\n",
        "\n",
        "Parameter `max_depth` membatasi kedalaman maksimum pohon.\n",
        "\n",
        "- Nilai kecil → pohon lebih sederhana → risiko underfitting\n",
        "- Nilai besar → pohon lebih kompleks → risiko overfitting\n",
        "\n",
        "Mengatur `max_depth` merupakan salah satu cara paling efektif untuk mengontrol kompleksitas Decision Tree."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Minimum Samples for a Split (`min_samples_split`)\n",
        "\n",
        "Parameter `min_samples_split` menentukan jumlah minimum instance yang dibutuhkan agar sebuah node dapat di-split.\n",
        "\n",
        "Nilai yang lebih besar akan mencegah pemisahan node yang hanya berisi sedikit data, sehingga mengurangi overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3 Minimum Samples per Leaf (`min_samples_leaf`)\n",
        "\n",
        "`min_samples_leaf` menentukan jumlah minimum instance yang harus ada pada setiap leaf node.\n",
        "\n",
        "Parameter ini sangat efektif untuk menghaluskan prediksi, karena mencegah leaf node yang hanya berisi satu atau dua instance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.4 Maximum Features (`max_features`)\n",
        "\n",
        "Parameter `max_features` membatasi jumlah fitur yang dipertimbangkan saat mencari split terbaik.\n",
        "\n",
        "Dengan membatasi fitur, model menjadi lebih sederhana dan kurang sensitif terhadap noise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.5 Example: Regularized Decision Tree\n",
        "\n",
        "Berikut contoh Decision Tree dengan beberapa parameter regularization yang diaktifkan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tree_clf_reg = DecisionTreeClassifier(\n",
        "    max_depth=3,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "tree_clf_reg.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dengan regularization, Decision Tree menjadi lebih stabil dan memiliki kemampuan generalisasi yang lebih baik pada data baru.\n",
        "\n",
        "Pemilihan nilai hyperparameter biasanya dilakukan melalui cross-validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Instability of Decision Trees\n",
        "\n",
        "Meskipun Decision Tree sangat intuitif dan mudah diinterpretasikan, model ini memiliki kelemahan penting, yaitu **sensitivitas yang tinggi terhadap perubahan kecil pada data**.\n",
        "\n",
        "Perubahan kecil pada dataset (misalnya penambahan atau penghapusan beberapa instance) dapat menghasilkan struktur pohon yang sangat berbeda."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.1 Sensitivity to Data Variations\n",
        "\n",
        "Karena Decision Tree membuat keputusan berdasarkan split terbaik secara lokal, sedikit perubahan pada data dapat mengubah split awal.\n",
        "\n",
        "Perubahan ini kemudian *berantai* ke seluruh struktur pohon, menghasilkan model yang sangat berbeda meskipun datanya hampir sama."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.2 Sensitivity to Axis Orientation\n",
        "\n",
        "Decision Tree juga sensitif terhadap orientasi sumbu fitur.\n",
        "\n",
        "Jika data diputar (misalnya melalui transformasi linier), Decision Tree mungkin memerlukan lebih banyak split untuk memisahkan data dengan baik.\n",
        "\n",
        "Hal ini terjadi karena Decision Tree membuat split **sejajar sumbu fitur** (*axis-aligned splits*)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Kelemahan ini menjadi motivasi utama pengembangan metode ensemble seperti **Random Forest**, yang menggabungkan banyak Decision Tree untuk mengurangi variance dan meningkatkan stabilitas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Closing Summary (Chapter 6)\n",
        "\n",
        "Pada Chapter 6, kita mempelajari Decision Trees sebagai algoritma Machine Learning yang kuat dan mudah diinterpretasikan.\n",
        "\n",
        "Konsep utama yang dibahas meliputi:\n",
        "- cara melatih dan memvisualisasikan Decision Tree,\n",
        "- mekanisme prediksi dan perhitungan probabilitas kelas,\n",
        "- kriteria split seperti Gini impurity dan entropy,\n",
        "- regularization untuk mencegah overfitting,\n",
        "- penggunaan Decision Tree untuk regresi,\n",
        "- serta keterbatasan Decision Tree terkait stabilitas.\n",
        "\n",
        "Meskipun memiliki kelemahan, Decision Tree menjadi fondasi penting bagi algoritma ensemble modern.\n",
        "\n",
        "Pemahaman yang baik terhadap Decision Tree mempersiapkan kita untuk mempelajari model yang lebih kuat dan stabil pada chapter berikutnya, seperti **Ensemble Learning dan Random Forests**."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
