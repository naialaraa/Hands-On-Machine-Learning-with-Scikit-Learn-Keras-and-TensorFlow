{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
        "\n",
        "Pada chapter ini, kita mulai memasuki inti dari **Deep Learning**, yaitu **Artificial Neural Networks (ANN)**.\n",
        "\n",
        "Chapter ini sangat penting karena menjadi jembatan antara Machine Learning klasik (seperti regresi, SVM, dan decision tree) menuju model yang lebih kompleks dan fleksibel, yaitu jaringan saraf tiruan.\n",
        "\n",
        "Fokus utama chapter ini adalah:\n",
        "- memahami asal-usul dan intuisi neural network,\n",
        "- mengenal komponen dasar ANN seperti neuron, layer, dan activation function,\n",
        "- memahami bagaimana **Multilayer Perceptron (MLP)** bekerja,\n",
        "- serta mengimplementasikan ANN menggunakan **Keras (tf.keras)**.\n",
        "\n",
        "Chapter ini juga menyiapkan fondasi konseptual untuk chapter selanjutnya yang akan membahas **Deep Neural Networks**, **training tricks**, dan **advanced architectures**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Dari Neuron Biologis ke Neuron Artifisial\n",
        "\n",
        "Ide dasar neural network terinspirasi dari **cara kerja otak manusia**.\n",
        "\n",
        "Otak manusia terdiri dari miliaran neuron biologis yang saling terhubung. Masing-masing neuron:\n",
        "- menerima sinyal dari neuron lain,\n",
        "- mengolah sinyal tersebut,\n",
        "- lalu mengirimkan sinyal ke neuron berikutnya.\n",
        "\n",
        "Meskipun satu neuron terlihat sederhana, kombinasi jutaan neuron mampu menghasilkan perilaku yang sangat kompleks, seperti pengenalan pola, bahasa, dan pengambilan keputusan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Artificial Neural Network (ANN) mencoba meniru **konsep dasarnya**, bukan meniru otak secara biologis penuh.\n",
        "\n",
        "ANN modern tidak harus realistis secara biologis. Yang terpenting adalah:\n",
        "- model tersebut **bisa belajar dari data**,\n",
        "- mampu melakukan generalisasi,\n",
        "- dan memberikan performa yang baik pada masalah kompleks.\n",
        "\n",
        "Hal ini mirip dengan pesawat terbang: terinspirasi dari burung, tetapi tidak mengepakkan sayap."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Neuron Artifisial Awal (McCulloch–Pitts Neuron)\n",
        "\n",
        "Model neuron artifisial pertama diperkenalkan pada tahun 1943 oleh **McCulloch dan Pitts**.\n",
        "\n",
        "Neuron ini:\n",
        "- memiliki input biner (0 atau 1),\n",
        "- menghasilkan output biner,\n",
        "- aktif jika jumlah input aktif melebihi suatu ambang batas.\n",
        "\n",
        "Walaupun sangat sederhana, neuron ini mampu merepresentasikan operasi logika seperti **AND**, **OR**, dan **NOT**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dengan menggabungkan banyak neuron sederhana, kita dapat membangun jaringan yang mampu merepresentasikan ekspresi logika yang lebih kompleks.\n",
        "\n",
        "Konsep ini menunjukkan bahwa:\n",
        "- **kompleksitas dapat muncul dari unit-unit sederhana**,\n",
        "- struktur jaringan lebih penting daripada kecanggihan satu neuron.\n",
        "\n",
        "Gagasan ini menjadi fondasi utama perkembangan neural network modern."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Perceptron\n",
        "\n",
        "**Perceptron** adalah salah satu arsitektur ANN paling awal dan sederhana, diperkenalkan oleh **Frank Rosenblatt (1957)**.\n",
        "\n",
        "Perceptron menggunakan neuron yang disebut **Threshold Logic Unit (TLU)**, yang:\n",
        "- menerima input numerik,\n",
        "- mengalikan setiap input dengan bobot (weight),\n",
        "- menjumlahkannya,\n",
        "- lalu menerapkan fungsi ambang (step function)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Secara intuitif, TLU bekerja seperti ini:\n",
        "\n",
        "1. Hitung kombinasi linear input:\n",
        "   $$z = w_1 x_1 + w_2 x_2 + \\dots + w_n x_n + b$$\n",
        "2. Terapkan fungsi aktivasi berbentuk step:\n",
        "   - jika $z \\geq 0$ → output = 1\n",
        "   - jika $z < 0$ → output = 0\n",
        "\n",
        "Perceptron dapat digunakan untuk **klasifikasi linear**, mirip dengan Logistic Regression atau Linear SVM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Training a Perceptron\n",
        "\n",
        "Salah satu keunggulan utama perceptron adalah adanya **aturan pembelajaran (training rule)** yang sederhana dan intuitif.\n",
        "\n",
        "Tujuan training perceptron adalah mencari bobot (weights) dan bias yang mampu memisahkan data ke dalam kelas-kelas yang benar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Perceptron Learning Rule\n",
        "\n",
        "Perceptron dilatih secara **iteratif** menggunakan contoh data satu per satu.\n",
        "\n",
        "Untuk setiap data:\n",
        "- jika prediksi benar → bobot tidak diubah,\n",
        "- jika prediksi salah → bobot diperbarui.\n",
        "\n",
        "Pembaruan bobot dilakukan dengan aturan sederhana:\n",
        "\n",
        "$$w_i \\leftarrow w_i + \\eta (y - \\hat{y}) x_i$$\n",
        "\n",
        "di mana:\n",
        "- $\\eta$ adalah learning rate,\n",
        "- $y$ adalah label sebenarnya,\n",
        "- $\\hat{y}$ adalah prediksi perceptron."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Intuisi dari aturan ini adalah:\n",
        "- jika model memprediksi terlalu rendah, bobot ditingkatkan,\n",
        "- jika model memprediksi terlalu tinggi, bobot dikurangi.\n",
        "\n",
        "Dengan cara ini, decision boundary akan bergeser secara bertahap menuju posisi yang lebih tepat."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Konvergensi Perceptron\n",
        "\n",
        "Perceptron memiliki properti penting:\n",
        "\n",
        "> Jika data **linearly separable**, maka perceptron **pasti akan konvergen** dalam jumlah iterasi terbatas.\n",
        "\n",
        "Namun, jika data **tidak linearly separable**, perceptron **tidak akan pernah konvergen** dan bobot akan terus berubah."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Implementasi Perceptron dengan Scikit-Learn\n",
        "\n",
        "Scikit-Learn menyediakan implementasi perceptron melalui class `Perceptron`.\n",
        "\n",
        "Implementasi ini sangat berguna untuk eksperimen dan pemahaman konsep dasar neural network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data[:, (2, 3)]\n",
        "y = (iris.target == 0).astype(int)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "perceptron = Perceptron(random_state=42)\n",
        "perceptron.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model perceptron di atas dilatih untuk membedakan kelas Iris Setosa dari kelas lainnya.\n",
        "\n",
        "Perceptron hanya mampu menemukan **decision boundary linier**, sehingga performanya sangat bergantung pada apakah data dapat dipisahkan secara linier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Keterbatasan Perceptron\n",
        "\n",
        "Meskipun penting secara historis, perceptron memiliki keterbatasan besar:\n",
        "- hanya mampu mempelajari pola linier,\n",
        "- tidak dapat menyelesaikan masalah non-linear seperti XOR,\n",
        "- tidak menghasilkan probabilitas keluaran.\n",
        "\n",
        "Keterbatasan ini sempat menyebabkan penurunan minat terhadap neural network pada akhir tahun 1960-an."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Keterbatasan perceptron inilah yang mendorong pengembangan:\n",
        "- **Multilayer Perceptron (MLP)**,\n",
        "- fungsi aktivasi non-linear,\n",
        "- dan algoritma **backpropagation**.\n",
        "\n",
        "Topik-topik tersebut akan dibahas pada bagian selanjutnya."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Multilayer Perceptron (MLP)\n",
        "\n",
        "Untuk mengatasi keterbatasan perceptron tunggal, dikembangkan arsitektur yang disebut **Multilayer Perceptron (MLP)**.\n",
        "\n",
        "MLP terdiri dari beberapa layer neuron yang disusun berurutan, yaitu:\n",
        "- **input layer**: menerima fitur dari data,\n",
        "- **hidden layer(s)**: melakukan transformasi non-linear,\n",
        "- **output layer**: menghasilkan prediksi akhir.\n",
        "\n",
        "Dengan adanya hidden layer dan fungsi aktivasi non-linear, MLP mampu mempelajari pola **non-linear** yang tidak dapat ditangani oleh perceptron tunggal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.1 Peran Hidden Layer\n",
        "\n",
        "Hidden layer memungkinkan jaringan saraf membangun **representasi bertingkat (hierarchical representations)**.\n",
        "\n",
        "Layer awal biasanya mempelajari pola sederhana, sedangkan layer yang lebih dalam mempelajari pola yang lebih kompleks.\n",
        "\n",
        "Inilah alasan mengapa neural network sangat efektif untuk data kompleks seperti gambar, suara, dan teks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Secara matematis, setiap neuron dalam MLP melakukan operasi:\n",
        "\n",
        "$$z = w_1 x_1 + w_2 x_2 + \\dots + w_n x_n + b$$\n",
        "\n",
        "Kemudian menerapkan fungsi aktivasi non-linear:\n",
        "\n",
        "$$a = f(z)$$\n",
        "\n",
        "Fungsi aktivasi inilah yang memberi kemampuan non-linear pada jaringan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Backpropagation\n",
        "\n",
        "**Backpropagation** adalah algoritma inti yang digunakan untuk melatih MLP.\n",
        "\n",
        "Algoritma ini memungkinkan jaringan saraf memperbarui bobot secara efisien dengan menghitung **gradien error** terhadap setiap bobot di seluruh layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.1 Intuisi Backpropagation\n",
        "\n",
        "Backpropagation bekerja dengan dua tahap utama:\n",
        "\n",
        "1. **Forward pass**: data dilewatkan dari input layer hingga output untuk menghasilkan prediksi.\n",
        "2. **Backward pass**: error pada output dihitung dan disebarkan kembali ke layer sebelumnya untuk memperbarui bobot.\n",
        "\n",
        "Proses ini menggunakan aturan rantai (*chain rule*) dari kalkulus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Intuisi sederhananya:\n",
        "- jika output terlalu besar → bobot tertentu perlu dikurangi,\n",
        "- jika output terlalu kecil → bobot tertentu perlu ditingkatkan.\n",
        "\n",
        "Setiap bobot diperbarui secara proporsional terhadap kontribusinya pada error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Universal Approximation Theorem\n",
        "\n",
        "Salah satu hasil teoretis penting dalam neural network adalah **Universal Approximation Theorem**.\n",
        "\n",
        "Teorema ini menyatakan bahwa:\n",
        "\n",
        "> Sebuah MLP dengan **satu hidden layer** yang cukup besar mampu mendekati fungsi kontinu apa pun dengan tingkat akurasi tertentu.\n",
        "\n",
        "Artinya, MLP memiliki kapasitas representasi yang sangat kuat."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Namun, dalam praktik:\n",
        "- menggunakan **beberapa hidden layer** sering kali lebih efisien,\n",
        "- jaringan yang terlalu besar berisiko overfitting,\n",
        "- training menjadi lebih stabil dengan arsitektur yang tepat.\n",
        "\n",
        "Inilah alasan mengapa *deep learning* (banyak layer) menjadi pendekatan dominan saat ini."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Activation Functions\n",
        "\n",
        "Fungsi aktivasi merupakan komponen kunci dalam neural network.\n",
        "\n",
        "Tanpa fungsi aktivasi non-linear, jaringan saraf dengan banyak layer akan setara dengan satu model linear saja, sehingga tidak mampu mempelajari pola kompleks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fungsi aktivasi menentukan bagaimana sinyal input yang telah dikombinasikan secara linear akan diubah menjadi output neuron.\n",
        "\n",
        "Dengan kata lain, fungsi aktivasi menentukan **apakah dan seberapa kuat sebuah neuron akan aktif**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10.1 Sigmoid Function\n",
        "\n",
        "Fungsi **sigmoid** merupakan salah satu fungsi aktivasi paling awal yang digunakan dalam neural network.\n",
        "\n",
        "Sigmoid memetakan nilai input ke rentang (0, 1), sehingga sering diinterpretasikan sebagai probabilitas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Secara matematis, fungsi sigmoid didefinisikan sebagai:\n",
        "\n",
        "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
        "\n",
        "Sigmoid banyak digunakan pada:\n",
        "- output layer untuk klasifikasi biner,\n",
        "- model-model awal neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Namun, sigmoid memiliki kelemahan utama:\n",
        "- **vanishing gradient** pada nilai ekstrem,\n",
        "- output tidak berpusat di nol,\n",
        "- training menjadi lambat pada jaringan dalam.\n",
        "\n",
        "Karena alasan ini, sigmoid jarang digunakan pada hidden layer modern."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10.2 ReLU (Rectified Linear Unit)\n",
        "\n",
        "**ReLU** merupakan fungsi aktivasi yang paling populer dalam deep learning saat ini.\n",
        "\n",
        "ReLU didefinisikan sebagai:\n",
        "\n",
        "$$\\text{ReLU}(z) = \\max(0, z)$$\n",
        "\n",
        "ReLU sederhana, efisien, dan membantu mengurangi masalah vanishing gradient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Kelebihan ReLU:\n",
        "- komputasi sangat cepat,\n",
        "- mempercepat konvergensi training,\n",
        "- bekerja baik pada jaringan dalam.\n",
        "\n",
        "Namun, ReLU juga memiliki kelemahan yaitu **dying ReLU**, di mana neuron berhenti aktif jika bobotnya terus menghasilkan output negatif."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10.3 Softmax Function\n",
        "\n",
        "Fungsi **softmax** biasanya digunakan pada output layer untuk **klasifikasi multikelas**.\n",
        "\n",
        "Softmax mengubah vektor skor menjadi distribusi probabilitas yang jumlahnya sama dengan 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Secara matematis:\n",
        "\n",
        "$$\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}$$\n",
        "\n",
        "Output softmax memudahkan interpretasi hasil model sebagai probabilitas untuk setiap kelas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pemilihan fungsi aktivasi sangat bergantung pada konteks:\n",
        "- **Sigmoid**: output biner,\n",
        "- **ReLU**: hidden layer,\n",
        "- **Softmax**: output multikelas.\n",
        "\n",
        "Pemilihan yang tepat dapat meningkatkan stabilitas dan performa training secara signifikan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Membangun Multilayer Perceptron dengan Keras\n",
        "\n",
        "Setelah memahami konsep dasar neural network dan fungsi aktivasi, langkah berikutnya adalah membangun **Multilayer Perceptron (MLP)** secara praktis.\n",
        "\n",
        "Pada chapter ini, framework yang digunakan adalah **Keras**, yang merupakan high-level API untuk membangun dan melatih neural network dengan mudah dan efisien."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Keras menyediakan beberapa pendekatan untuk membangun model, dan yang paling sederhana adalah **Sequential API**.\n",
        "\n",
        "Sequential API cocok digunakan ketika:\n",
        "- model berbentuk tumpukan layer secara berurutan,\n",
        "- setiap layer hanya memiliki satu input dan satu output,\n",
        "- tidak ada arsitektur bercabang atau skip connection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 11.1 Dataset MNIST\n",
        "\n",
        "Sebagai contoh, kita akan menggunakan dataset **MNIST**, yang berisi gambar tulisan tangan angka 0–9.\n",
        "\n",
        "Setiap gambar berukuran 28×28 piksel dan direpresentasikan sebagai vektor fitur numerik.\n",
        "\n",
        "Dataset ini sering digunakan sebagai *hello world* dalam deep learning karena cukup sederhana namun tetap menantang."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data MNIST terdiri dari:\n",
        "- `X_train`: gambar training,\n",
        "- `y_train`: label training,\n",
        "- `X_test`: gambar testing,\n",
        "- `y_test`: label testing.\n",
        "\n",
        "Sebelum digunakan, data perlu diproses agar sesuai dengan input neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 11.2 Preprocessing Data\n",
        "\n",
        "Langkah preprocessing utama adalah:\n",
        "- menormalisasi nilai piksel ke rentang [0, 1],\n",
        "- mengubah label menjadi format yang sesuai.\n",
        "\n",
        "Normalisasi membantu mempercepat dan menstabilkan proses training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 11.3 Membangun Arsitektur Model\n",
        "\n",
        "Model MLP dibangun menggunakan beberapa layer:\n",
        "- **Flatten layer** untuk mengubah gambar 2D menjadi vektor 1D,\n",
        "- **Dense hidden layers** dengan fungsi aktivasi ReLU,\n",
        "- **Dense output layer** dengan fungsi aktivasi Softmax untuk klasifikasi multikelas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(300, activation=\"relu\"),\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Arsitektur di atas merepresentasikan MLP dengan dua hidden layer.\n",
        "\n",
        "Jumlah neuron dan layer dapat disesuaikan tergantung kompleksitas masalah dan ketersediaan data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Compiling the Neural Network\n",
        "\n",
        "Setelah arsitektur model didefinisikan, langkah selanjutnya adalah **compile** model.\n",
        "\n",
        "Proses compile bertujuan untuk:\n",
        "- menentukan **loss function** yang akan diminimalkan,\n",
        "- memilih **optimizer** untuk memperbarui bobot,\n",
        "- menentukan **metrics** untuk evaluasi performa model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pada kasus klasifikasi multikelas seperti MNIST:\n",
        "- loss function yang umum digunakan adalah `sparse_categorical_crossentropy`,\n",
        "- optimizer yang sering digunakan adalah `SGD` atau `Adam`,\n",
        "- metric evaluasi yang paling umum adalah `accuracy`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    optimizer=\"sgd\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pemilihan konfigurasi di atas sesuai dengan contoh pada buku:\n",
        "- `SGD` (Stochastic Gradient Descent) digunakan untuk menunjukkan konsep dasar optimisasi,\n",
        "- loss function disesuaikan dengan format label integer pada dataset MNIST."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Training the Model\n",
        "\n",
        "Setelah model di-compile, proses selanjutnya adalah **training**.\n",
        "\n",
        "Training dilakukan menggunakan method `fit()`, yang akan:\n",
        "- melakukan forward pass,\n",
        "- menghitung loss,\n",
        "- melakukan backpropagation,\n",
        "- memperbarui bobot secara iteratif."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=30,\n",
        "    validation_split=0.1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Selama training, Keras menampilkan:\n",
        "- nilai loss dan accuracy pada data training,\n",
        "- nilai loss dan accuracy pada data validation.\n",
        "\n",
        "Validation set digunakan untuk memantau apakah model mengalami **overfitting**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Objek `history` menyimpan seluruh riwayat training, termasuk nilai loss dan accuracy pada setiap epoch.\n",
        "\n",
        "Informasi ini sangat berguna untuk analisis dan visualisasi proses training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Evaluating the Model\n",
        "\n",
        "Setelah training selesai, model dievaluasi menggunakan data **test** yang benar-benar belum pernah dilihat sebelumnya.\n",
        "\n",
        "Evaluasi ini memberikan gambaran performa generalisasi model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.evaluate(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hasil evaluasi biasanya menunjukkan bahwa MLP sederhana sudah mampu mencapai akurasi yang cukup tinggi pada dataset MNIST.\n",
        "\n",
        "Hal ini menunjukkan kekuatan neural network bahkan dengan arsitektur yang relatif sederhana."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. Making Predictions with the Trained Model\n",
        "\n",
        "Setelah model selesai dilatih dan dievaluasi, langkah berikutnya adalah menggunakan model tersebut untuk **membuat prediksi** pada data baru.\n",
        "\n",
        "Pada kasus MNIST, prediksi dilakukan untuk menentukan angka apa yang paling mungkin direpresentasikan oleh sebuah gambar tulisan tangan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Keras menyediakan method `predict()` untuk menghasilkan output model.\n",
        "\n",
        "Karena output layer menggunakan fungsi aktivasi **softmax**, hasil prediksi berupa **probabilitas untuk setiap kelas**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_proba = model.predict(X_test[:5])\n",
        "y_proba"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Setiap baris pada output di atas merepresentasikan distribusi probabilitas untuk kelas 0 hingga 9.\n",
        "\n",
        "Kelas dengan probabilitas tertinggi merupakan prediksi akhir model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = y_proba.argmax(axis=1)\n",
        "y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hasil `argmax` memberikan label kelas dengan probabilitas tertinggi untuk setiap gambar.\n",
        "\n",
        "Prediksi ini kemudian dapat dibandingkan dengan label sebenarnya untuk analisis lebih lanjut."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Closing Summary (Chapter 10)\n",
        "\n",
        "Chapter 10 merupakan pengenalan fundamental terhadap **Artificial Neural Networks (ANN)** dan menjadi pintu masuk ke dunia **Deep Learning**.\n",
        "\n",
        "Pada chapter ini, kita mempelajari:\n",
        "- inspirasi biologis di balik neural network,\n",
        "- perceptron dan keterbatasannya,\n",
        "- Multilayer Perceptron (MLP) sebagai solusi pola non-linear,\n",
        "- konsep backpropagation sebagai algoritma training utama,\n",
        "- fungsi aktivasi seperti sigmoid, ReLU, dan softmax,\n",
        "- serta implementasi MLP menggunakan Keras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Melalui contoh dataset MNIST, chapter ini menunjukkan bagaimana:\n",
        "- neural network dibangun secara bertahap,\n",
        "- proses training dan evaluasi dilakukan,\n",
        "- serta bagaimana model digunakan untuk membuat prediksi nyata.\n",
        "\n",
        "Contoh ini memberikan gambaran praktis end-to-end mengenai workflow deep learning sederhana."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
