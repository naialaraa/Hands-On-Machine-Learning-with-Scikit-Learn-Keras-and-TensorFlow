{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 16: Natural Language Processing with RNNs and Attention\n",
    "\n",
    "Bab ini mengeksplorasi teknik pemrosesan bahasa alami (NLP) yang lebih maju. Kita akan beralih dari sekadar memproses urutan angka sederhana ke memproses teks, di mana konteks dan hubungan antar kata sangat krusial.\n",
    "\n",
    "## Tujuan Pembelajaran:\n",
    "1. **Membangun Char-RNN**: Menghasilkan teks secara kreatif.\n",
    "2. **Memahami Sentiment Analysis**: Klasifikasi teks menggunakan Embedding.\n",
    "3. **Eksplorasi Encoder-Decoder**: Dasar dari sistem terjemahan mesin.\n",
    "4. **Mekanisme Attention**: Fokus pada bagian data yang relevan.\n",
    "5. **Transformer**: Arsitektur modern yang mendasari model seperti GPT dan BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Menghasilkan Teks Karakter demi Karakter (Char-RNN)\n",
    "\n",
    "Char-RNN dilatih untuk memprediksi karakter berikutnya dalam urutan teks. Ini memungkinkan model untuk mempelajari struktur bahasa dari tingkat paling dasar (huruf, spasi, tanda baca).\n",
    "\n",
    "### Proses Data:\n",
    "- **Tokenisasi**: Mengubah karakter menjadi ID numerik.\n",
    "- **Sliding Window**: Memotong teks panjang menjadi urutan kecil untuk latihan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "# 1. Tokenisasi tingkat karakter\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "shakespeare_text = \"To be, or not to be, that is the question.\"\n",
    "tokenizer.fit_on_texts([shakespeare_text])\n",
    "\n",
    "max_id = len(tokenizer.word_index) # Jumlah karakter unik\n",
    "dataset_size = len(tokenizer.texts_to_sequences([shakespeare_text])[0])\n",
    "\n",
    "print(f\"Jumlah karakter unik: {max_id}\")\n",
    "print(f\"Total panjang teks: {dataset_size}\")\n",
    "\n",
    "# 2. Membangun model GRU untuk Char-RNN\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id], dropout=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True, dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=\"softmax\"))\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "print(\"Model Char-RNN siap dilatih.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stateful vs Stateless RNN\n",
    "\n",
    "| Fitur | Stateless RNN | Stateful RNN |\n",
    "|---|---|---|\n",
    "| **Memory** | Reset di setiap batch | Dipertahankan antar batch |\n",
    "| **Urutan Data** | Boleh diacak | Harus berurutan |\n",
    "| **Konteks** | Jangka pendek | Jangka sangat panjang |\n",
    "\n",
    "Stateful RNN sangat berguna jika kita ingin model mengingat konteks dari ribuan langkah waktu sebelumnya, namun memerlukan manajemen batch yang sangat disiplin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sentiment Analysis (Analisis Sentimen)\n",
    "\n",
    "Dalam analisis sentimen, kita mengubah kata menjadi vektor padat menggunakan lapisan **Embedding**. Kita juga menggunakan **Masking** agar model mengabaikan padding (token kosong) yang digunakan untuk menyamakan panjang kalimat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Analisis Sentimen untuk dataset IMDb\n",
    "model_sentiment = keras.models.Sequential([\n",
    "    # input_dim: ukuran kosakata, output_dim: dimensi vektor embedding\n",
    "    keras.layers.Embedding(input_dim=10000, output_dim=128, mask_zero=True, input_shape=[None]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model_sentiment.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "print(\"Model Sentiment Analysis berhasil dikonfigurasi dengan Masking.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Encoder-Decoder untuk Terjemahan Mesin\n",
    "\n",
    "Arsitektur ini menggunakan dua bagian utama:\n",
    "1. **Encoder**: Memproses kalimat bahasa asal (misal: Inggris) menjadi satu vektor konteks.\n",
    "2. **Decoder**: Mengambil vektor tersebut dan menghasilkan kalimat bahasa tujuan (misal: Indonesia) kata demi kata.\n",
    "\n",
    "**Bottleneck**: Vektor konteks tunggal sulit menampung informasi dari kalimat yang sangat panjang."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Mekanisme Attention\n",
    "\n",
    "Attention memecahkan masalah bottleneck dengan membiarkan Decoder \"melihat\" seluruh urutan output dari Encoder dan memilih bagian mana yang paling relevan untuk kata yang sedang diterjemahkan saat ini.\n",
    "\n",
    "- **Bahdanau Attention**: Sering disebut 'Additive Attention'.\n",
    "- **Luong Attention**: Sering disebut 'Multiplicative Attention'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Transformer: Attention Is All You Need\n",
    "\n",
    "Transformer adalah arsitektur yang membuang RNN sepenuhnya dan mengandalkan **Self-Attention**. Ini memungkinkan pelatihan yang jauh lebih cepat karena data diproses secara paralel (tidak sekuensial).\n",
    "\n",
    "### Komponen Kunci Transformer:\n",
    "- **Multi-Head Attention**: Melihat hubungan kata dari berbagai perspektif.\n",
    "- **Positional Encoding**: Menambahkan informasi urutan karena Transformer tidak memproses data secara berurutan.\n",
    "- **Residual Connections**: Memudahkan aliran gradien pada network yang dalam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh implementasi Multi-Head Attention dari Keras\n",
    "num_heads = 8\n",
    "key_dim = 64\n",
    "mha_layer = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)\n",
    "\n",
    "# Simulasi input (batch_size, sequence_length, embedding_dim)\n",
    "query = tf.random.uniform((32, 50, 512))\n",
    "value = query # Self-attention\n",
    "\n",
    "output_tensor = mha_layer(query, value)\n",
    "print(f\"Output shape dari Multi-Head Attention: {output_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Bahasa Modern (GPT & BERT)\n",
    "\n",
    "- **BERT (Bidirectional Encoder Representations from Transformers)**: Dilatih untuk memahami konteks dari dua arah (kiri ke kanan dan kanan ke kiri). Sangat bagus untuk pemahaman teks.\n",
    "- **GPT (Generative Pre-trained Transformer)**: Berfokus pada generasi teks (memprediksi kata berikutnya). Dilatih hanya dari satu arah (kiri ke kanan).\n",
    "\n",
    "## Ringkasan Bab 16\n",
    "1. RNN sangat kuat untuk urutan, tetapi memiliki batas memori jangka panjang.\n",
    "2. **Embedding** dan **Masking** adalah dasar dari pengolahan teks modern.\n",
    "3. **Attention** merevolusi cara kita menangani urutan panjang.\n",
    "4. **Transformer** saat ini menjadi standar emas dalam dunia NLP, mengungguli RNN dalam hampir semua tugas besar."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
