{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 7: Ensemble Learning and Random Forests\n",
        "\n",
        "Pada chapter ini, kita mempelajari **Ensemble Learning**, yaitu pendekatan Machine Learning yang menggabungkan beberapa model untuk menghasilkan prediksi yang lebih akurat dan stabil dibandingkan model tunggal.\n",
        "\n",
        "Ide utama dari Ensemble Learning adalah bahwa **sekumpulan model yang beragam** sering kali mampu memberikan prediksi yang lebih baik daripada satu model terbaik sekalipun.\n",
        "\n",
        "Pendekatan ini banyak digunakan pada tahap akhir proyek Machine Learning dan menjadi fondasi berbagai algoritma canggih yang sering memenangkan kompetisi Machine Learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Ensemble Learning: Intuisi Dasar\n",
        "\n",
        "Ensemble Learning terinspirasi dari konsep **wisdom of the crowd**, yaitu gagasan bahwa keputusan kolektif dari banyak individu sering kali lebih baik dibandingkan keputusan satu ahli.\n",
        "\n",
        "Dalam konteks Machine Learning:\n",
        "- setiap model disebut sebagai *predictor* atau *learner*,\n",
        "- sekumpulan predictor disebut sebagai **ensemble**,\n",
        "- prediksi akhir diperoleh dengan menggabungkan prediksi semua predictor.\n",
        "\n",
        "Jika masing-masing model memiliki performa yang cukup baik dan membuat kesalahan yang berbeda, maka kesalahan tersebut dapat saling menutupi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Weak Learners vs Strong Learners\n",
        "\n",
        "Dalam Ensemble Learning, sering kali digunakan **weak learners**, yaitu model yang performanya hanya sedikit lebih baik daripada tebakan acak.\n",
        "\n",
        "Meskipun setiap model secara individual lemah, penggabungan banyak weak learners dapat menghasilkan **strong learner** dengan performa tinggi.\n",
        "\n",
        "Kunci keberhasilan pendekatan ini adalah **diversitas**, yaitu setiap model membuat jenis kesalahan yang berbeda."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Mengapa Ensemble Bisa Lebih Baik?\n",
        "\n",
        "Secara intuitif:\n",
        "- jika beberapa model membuat kesalahan secara bersamaan, ensemble tetap bisa salah,\n",
        "- namun jika kesalahan tiap model tidak berkorelasi kuat, ensemble cenderung memilih prediksi yang benar melalui mekanisme agregasi.\n",
        "\n",
        "Oleh karena itu, Ensemble Learning bekerja paling efektif ketika model-model di dalamnya **cukup akurat dan saling independen**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Chapter ini akan membahas beberapa teknik ensemble populer, antara lain:\n",
        "- Voting Classifiers,\n",
        "- Bagging dan Pasting,\n",
        "- Random Forests,\n",
        "- Boosting (AdaBoost dan Gradient Boosting),\n",
        "- serta Stacking.\n",
        "\n",
        "Seluruh teknik ini akan dijelaskan secara bertahap dengan intuisi, contoh kode, dan interpretasi hasil."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Voting Classifiers\n",
        "\n",
        "Salah satu bentuk Ensemble Learning yang paling sederhana adalah **Voting Classifier**.\n",
        "\n",
        "Ide utamanya adalah melatih beberapa classifier yang berbeda, kemudian menggabungkan prediksi mereka untuk menentukan prediksi akhir.\n",
        "\n",
        "Voting Classifier sangat efektif ketika model-model penyusunnya cukup akurat dan membuat kesalahan yang berbeda."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Hard Voting\n",
        "\n",
        "Pada **hard voting**, setiap classifier memberikan satu suara untuk kelas yang diprediksinya.\n",
        "\n",
        "Kelas yang memperoleh suara terbanyak akan dipilih sebagai prediksi akhir ensemble.\n",
        "\n",
        "Pendekatan ini sederhana dan bekerja dengan baik jika semua classifier memiliki performa yang relatif seimbang."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Berikut contoh penerapan hard voting menggunakan tiga classifier berbeda:\n",
        "- Logistic Regression\n",
        "- Random Forest\n",
        "- Support Vector Machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "log_clf = LogisticRegression()\n",
        "rnd_clf = RandomForestClassifier()\n",
        "svm_clf = SVC()\n",
        "\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
        "    voting='hard'\n",
        ")\n",
        "\n",
        "voting_clf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Voting Classifier di atas akan mengambil prediksi kelas dari masing-masing model, lalu menentukan kelas mayoritas sebagai output akhir."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Training and Evaluating the Voting Classifier\n",
        "\n",
        "Untuk mengevaluasi performa Voting Classifier, kita akan melatih masing-masing model dan membandingkan akurasinya."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = make_moons(n_samples=10000, noise=0.4, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hasil evaluasi biasanya menunjukkan bahwa Voting Classifier memiliki akurasi yang **setara atau lebih baik** dibandingkan masing-masing model individual.\n",
        "\n",
        "Hal ini menunjukkan kekuatan agregasi prediksi dalam Ensemble Learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Soft Voting\n",
        "\n",
        "Pada **soft voting**, setiap classifier memberikan probabilitas untuk setiap kelas.\n",
        "\n",
        "Probabilitas tersebut kemudian dirata-ratakan, dan kelas dengan probabilitas rata-rata tertinggi dipilih sebagai prediksi akhir.\n",
        "\n",
        "Soft voting biasanya memberikan performa yang lebih baik dibandingkan hard voting, terutama jika model-model memiliki tingkat kepercayaan yang berbeda."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agar soft voting dapat digunakan, setiap classifier harus mampu menghasilkan probabilitas kelas.\n",
        "\n",
        "Sebagai contoh, Support Vector Machine perlu diinisialisasi dengan `probability=True`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "svm_clf = SVC(probability=True)\n",
        "\n",
        "voting_clf_soft = VotingClassifier(\n",
        "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
        "    voting='soft'\n",
        ")\n",
        "\n",
        "voting_clf_soft"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Soft voting mempertimbangkan tingkat keyakinan masing-masing model, sehingga sering kali menghasilkan prediksi yang lebih stabil dan akurat."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Bagging and Pasting\n",
        "\n",
        "Voting Classifier menggabungkan model yang dilatih pada dataset yang sama. Pendekatan lain untuk meningkatkan **diversitas model** adalah dengan melatih setiap model pada **subset data yang berbeda**.\n",
        "\n",
        "Pendekatan ini dikenal sebagai **Bagging** dan **Pasting**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Bagging (Bootstrap Aggregating)\n",
        "\n",
        "**Bagging** adalah teknik ensemble yang melatih beberapa model pada subset data training yang diambil **dengan pengembalian (with replacement)**.\n",
        "\n",
        "Artinya, satu instance data bisa muncul lebih dari satu kali dalam satu subset, sementara instance lain mungkin tidak muncul sama sekali.\n",
        "\n",
        "Pendekatan ini efektif untuk mengurangi **variance**, terutama pada model yang tidak stabil seperti Decision Tree."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Pasting\n",
        "\n",
        "**Pasting** mirip dengan Bagging, tetapi subset data diambil **tanpa pengembalian (without replacement)**.\n",
        "\n",
        "Akibatnya, setiap subset berisi instance yang unik. Pasting cenderung menghasilkan model yang sedikit kurang beragam dibandingkan Bagging."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Baik Bagging maupun Pasting bertujuan untuk melatih banyak model yang berbeda, kemudian menggabungkan prediksinya melalui voting (classification) atau averaging (regression)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Bagging with Scikit-Learn\n",
        "\n",
        "Scikit-Learn menyediakan class `BaggingClassifier` dan `BaggingRegressor` untuk menerapkan teknik ini secara langsung."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "bag_clf = BaggingClassifier(\n",
        "    DecisionTreeClassifier(),\n",
        "    n_estimators=500,\n",
        "    max_samples=100,\n",
        "    bootstrap=True,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "bag_clf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pada konfigurasi di atas:\n",
        "- `n_estimators=500` berarti 500 Decision Tree dilatih,\n",
        "- `max_samples=100` menentukan jumlah instance pada setiap subset,\n",
        "- `bootstrap=True` mengaktifkan Bagging (jika False â†’ Pasting).\n",
        "\n",
        "Parameter `n_jobs=-1` memungkinkan training paralel menggunakan semua core CPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.4 Training and Evaluating Bagging Classifier\n",
        "\n",
        "Kita dapat melatih Bagging Classifier dan membandingkan performanya dengan Decision Tree tunggal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bag_clf.fit(X_train, y_train)\n",
        "y_pred = bag_clf.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Biasanya, Bagging menghasilkan akurasi yang lebih tinggi dan lebih stabil dibandingkan Decision Tree tunggal.\n",
        "\n",
        "Hal ini terjadi karena Bagging secara signifikan mengurangi variance model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Out-of-Bag (OOB) Evaluation\n",
        "\n",
        "Salah satu keunggulan utama teknik Bagging adalah kemampuannya untuk melakukan evaluasi model **tanpa memerlukan validation set terpisah**.\n",
        "\n",
        "Pendekatan ini dikenal sebagai **Out-of-Bag (OOB) evaluation**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Apa itu Out-of-Bag Samples?\n",
        "\n",
        "Pada Bagging, setiap model dilatih menggunakan subset data yang diambil dengan pengembalian (*bootstrap sampling*).\n",
        "\n",
        "Akibatnya, sekitar **63%** dari data training digunakan untuk melatih setiap estimator, sementara **sekitar 37% sisanya tidak ikut terpilih**.\n",
        "\n",
        "Instance yang tidak terpilih ini disebut sebagai **out-of-bag samples**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Karena setiap instance training kemungkinan besar akan menjadi out-of-bag untuk beberapa estimator, kita dapat menggunakan prediksi dari estimator-estimator tersebut untuk mengevaluasi performa model.\n",
        "\n",
        "Dengan kata lain, OOB evaluation memberikan estimasi performa yang mirip dengan cross-validation, tetapi **lebih efisien secara komputasi**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Mengaktifkan OOB Evaluation di Scikit-Learn\n",
        "\n",
        "Untuk menggunakan OOB evaluation, kita hanya perlu mengatur parameter `oob_score=True` saat membuat BaggingClassifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bag_clf = BaggingClassifier(\n",
        "    DecisionTreeClassifier(),\n",
        "    n_estimators=500,\n",
        "    max_samples=100,\n",
        "    bootstrap=True,\n",
        "    n_jobs=-1,\n",
        "    oob_score=True\n",
        ")\n",
        "\n",
        "bag_clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Setelah model dilatih, kita dapat mengakses skor OOB melalui atribut `oob_score_`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bag_clf.oob_score_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nilai OOB score ini memberikan estimasi akurasi model pada data yang tidak digunakan saat training setiap estimator.\n",
        "\n",
        "Dalam banyak kasus, OOB score sangat dekat dengan hasil evaluasi pada test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Random Forests\n",
        "\n",
        "**Random Forest** merupakan salah satu algoritma ensemble paling populer dan kuat.\n",
        "\n",
        "Secara konsep, Random Forest adalah kumpulan **Decision Tree** yang dilatih menggunakan teknik **Bagging**, dengan tambahan sumber diversitas berupa **pemilihan fitur secara acak** pada setiap split."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 Mengapa Random Forest Bekerja dengan Baik?\n",
        "\n",
        "Decision Tree tunggal cenderung memiliki variance yang tinggi. Random Forest mengurangi masalah ini dengan:\n",
        "- melatih banyak pohon pada subset data yang berbeda (bootstrap sampling),\n",
        "- membatasi jumlah fitur yang dipertimbangkan pada setiap split.\n",
        "\n",
        "Kombinasi ini menghasilkan pohon-pohon yang **kurang berkorelasi**, sehingga prediksi gabungannya menjadi lebih stabil dan akurat."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Random Forest for Classification\n",
        "\n",
        "Untuk tugas klasifikasi, Random Forest menggabungkan prediksi setiap pohon menggunakan **majority voting**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rnd_clf = RandomForestClassifier(\n",
        "    n_estimators=500,\n",
        "    max_leaf_nodes=16,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rnd_clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model Random Forest di atas terdiri dari 500 Decision Tree dengan jumlah leaf node dibatasi untuk mencegah overfitting.\n",
        "\n",
        "Parameter `n_jobs=-1` memungkinkan training dilakukan secara paralel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3 Evaluating the Random Forest\n",
        "\n",
        "Setelah training, kita dapat mengevaluasi performa Random Forest pada test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred_rf = rnd_clf.predict(X_test)\n",
        "accuracy_score(y_test, y_pred_rf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Biasanya, Random Forest memberikan performa yang lebih baik dibandingkan Decision Tree tunggal karena variance yang lebih rendah."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.4 Random Forest vs Bagging\n",
        "\n",
        "Random Forest pada dasarnya adalah **Bagging Decision Trees** dengan tambahan randomisasi fitur.\n",
        "\n",
        "Perbedaan utama:\n",
        "- Bagging mempertimbangkan semua fitur pada setiap split,\n",
        "- Random Forest hanya mempertimbangkan subset fitur secara acak.\n",
        "\n",
        "Randomisasi fitur ini meningkatkan diversitas antar pohon."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Extra-Trees (Extremely Randomized Trees)\n",
        "\n",
        "Selain Random Forest, terdapat varian ensemble berbasis Decision Tree lain yang disebut **Extra-Trees** atau **Extremely Randomized Trees**.\n",
        "\n",
        "Extra-Trees memperkenalkan tingkat randomisasi yang lebih tinggi dibandingkan Random Forest, sehingga menghasilkan model yang lebih beragam."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 Perbedaan Extra-Trees dan Random Forest\n",
        "\n",
        "Perbedaan utama antara Extra-Trees dan Random Forest adalah:\n",
        "- Random Forest mencari split terbaik di antara subset fitur acak,\n",
        "- Extra-Trees memilih threshold split **secara acak**, tanpa mencari nilai terbaik.\n",
        "\n",
        "Akibatnya, Extra-Trees biasanya:\n",
        "- lebih cepat dilatih,\n",
        "- memiliki bias sedikit lebih tinggi,\n",
        "- tetapi variance yang lebih rendah."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Training an Extra-Trees Classifier\n",
        "\n",
        "Scikit-Learn menyediakan class `ExtraTreesClassifier` untuk mengimplementasikan metode ini."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "\n",
        "extra_trees_clf = ExtraTreesClassifier(\n",
        "    n_estimators=500,\n",
        "    max_leaf_nodes=16,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "extra_trees_clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model Extra-Trees di atas dilatih dengan konfigurasi yang mirip dengan Random Forest untuk memudahkan perbandingan performa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.3 Evaluating Extra-Trees\n",
        "\n",
        "Kita dapat mengevaluasi performa Extra-Trees pada test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred_extra = extra_trees_clf.predict(X_test)\n",
        "accuracy_score(y_test, y_pred_extra)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dalam banyak kasus, Extra-Trees dapat memberikan performa yang sebanding atau bahkan sedikit lebih baik dibandingkan Random Forest, dengan waktu training yang lebih singkat."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.4 Feature Importance\n",
        "\n",
        "Sama seperti Random Forest, Extra-Trees juga dapat digunakan untuk mengestimasi **feature importance**.\n",
        "\n",
        "Feature importance menunjukkan seberapa besar kontribusi setiap fitur dalam mengurangi impurity di seluruh pohon."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_importances = rnd_clf.feature_importances_\n",
        "feature_importances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nilai feature importance ini berguna untuk:\n",
        "- memahami fitur mana yang paling berpengaruh,\n",
        "- melakukan feature selection,\n",
        "- serta meningkatkan interpretabilitas model ensemble."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Boosting\n",
        "\n",
        "Berbeda dengan Bagging yang melatih model secara independen, **Boosting** melatih model **secara berurutan**.\n",
        "\n",
        "Setiap model baru berfokus untuk memperbaiki kesalahan yang dibuat oleh model sebelumnya.\n",
        "\n",
        "Dengan pendekatan ini, Boosting mampu mengubah sekumpulan **weak learners** menjadi model yang sangat kuat."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.1 Intuisi Boosting\n",
        "\n",
        "Secara intuitif:\n",
        "- model pertama dilatih pada seluruh data,\n",
        "- model berikutnya memberikan bobot lebih besar pada instance yang salah diprediksi,\n",
        "- proses ini berlanjut hingga terbentuk ensemble yang fokus pada kasus-kasus sulit.\n",
        "\n",
        "Boosting sangat efektif untuk mengurangi **bias**, tetapi rentan terhadap **overfitting** jika data sangat noisy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.2 AdaBoost (Adaptive Boosting)\n",
        "\n",
        "**AdaBoost** merupakan salah satu algoritma Boosting paling awal dan populer.\n",
        "\n",
        "Pada AdaBoost, setiap instance data memiliki bobot. Bobot instance yang salah diklasifikasikan akan ditingkatkan, sehingga model berikutnya lebih fokus pada instance tersebut."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.2.1 Training an AdaBoost Classifier\n",
        "\n",
        "Secara default, AdaBoost menggunakan Decision Tree dengan kedalaman 1 (*decision stump*) sebagai weak learner."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "ada_clf = AdaBoostClassifier(\n",
        "    DecisionTreeClassifier(max_depth=1),\n",
        "    n_estimators=200,\n",
        "    learning_rate=0.5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "ada_clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Parameter penting pada AdaBoost:\n",
        "- `n_estimators`: jumlah weak learners,\n",
        "- `learning_rate`: seberapa besar kontribusi setiap model baru.\n",
        "\n",
        "Learning rate yang kecil biasanya membutuhkan lebih banyak estimator, tetapi dapat meningkatkan generalisasi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.2.2 Evaluating AdaBoost\n",
        "\n",
        "Setelah training, kita dapat mengevaluasi performa AdaBoost pada test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred_ada = ada_clf.predict(X_test)\n",
        "accuracy_score(y_test, y_pred_ada)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dalam banyak kasus, AdaBoost mampu mengungguli Bagging dan Random Forest pada dataset tertentu, terutama ketika pola data relatif sederhana."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.3 Gradient Boosting\n",
        "\n",
        "**Gradient Boosting** merupakan pendekatan Boosting yang lebih umum dan fleksibel.\n",
        "\n",
        "Alih-alih menyesuaikan bobot instance secara eksplisit, Gradient Boosting melatih model baru untuk **memprediksi residual (kesalahan)** dari model sebelumnya."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.3.1 Intuisi Gradient Boosting\n",
        "\n",
        "Proses Gradient Boosting:\n",
        "1. model pertama memprediksi target,\n",
        "2. residual dihitung sebagai selisih antara target dan prediksi,\n",
        "3. model berikutnya dilatih untuk memprediksi residual tersebut,\n",
        "4. prediksi model-model digabungkan secara bertahap.\n",
        "\n",
        "Pendekatan ini mirip dengan optimisasi menggunakan *gradient descent*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.3.2 Training a Gradient Boosting Regressor\n",
        "\n",
        "Contoh berikut menunjukkan penggunaan Gradient Boosting untuk tugas regresi."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "gbrt = GradientBoostingRegressor(\n",
        "    max_depth=2,\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "gbrt.fit(X_reg, y_reg.ravel())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Gradient Boosting sangat powerful, tetapi juga sensitif terhadap hyperparameter.\n",
        "\n",
        "Regularization yang baik (misalnya dengan learning rate kecil dan early stopping) sangat penting untuk mencegah overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Stacking\n",
        "\n",
        "**Stacking** (stacked generalization) merupakan teknik ensemble yang lebih lanjut dibandingkan voting, bagging, atau boosting.\n",
        "\n",
        "Alih-alih hanya menggabungkan prediksi secara langsung, stacking menggunakan **model tambahan (meta-learner)** untuk mempelajari cara terbaik mengombinasikan prediksi dari beberapa model dasar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.1 Intuisi Stacking\n",
        "\n",
        "Pada stacking:\n",
        "- beberapa model dasar (*base learners*) dilatih pada data training,\n",
        "- setiap model menghasilkan prediksi,\n",
        "- prediksi-prediksi tersebut digunakan sebagai fitur input untuk model tingkat kedua (*meta-learner*).\n",
        "\n",
        "Meta-learner kemudian belajar bagaimana menimbang kontribusi masing-masing model dasar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pendekatan ini memungkinkan ensemble untuk:\n",
        "- memanfaatkan keunggulan setiap model dasar,\n",
        "- mengurangi kelemahan individual model,\n",
        "- menghasilkan prediksi yang lebih adaptif.\n",
        "\n",
        "Namun, stacking juga lebih kompleks dan berisiko overfitting jika tidak dirancang dengan hati-hati."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.2 Stacking in Practice\n",
        "\n",
        "Scikit-Learn menyediakan implementasi stacking melalui class `StackingClassifier` dan `StackingRegressor`.\n",
        "\n",
        "Pada praktiknya, meta-learner sering menggunakan model sederhana seperti Logistic Regression untuk menjaga generalisasi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Meskipun tidak selalu memberikan peningkatan performa yang signifikan, stacking sering digunakan dalam kompetisi Machine Learning dan sistem produksi berskala besar ketika setiap peningkatan kecil sangat bernilai."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Closing Summary (Chapter 7)\n",
        "\n",
        "Chapter 7 membahas berbagai teknik **Ensemble Learning** yang bertujuan meningkatkan performa dan stabilitas model Machine Learning.\n",
        "\n",
        "Konsep-konsep utama yang dipelajari meliputi:\n",
        "- Voting Classifiers (hard dan soft voting),\n",
        "- Bagging dan Out-of-Bag evaluation,\n",
        "- Random Forest dan Extra-Trees,\n",
        "- Boosting (AdaBoost dan Gradient Boosting),\n",
        "- serta Stacking sebagai teknik ensemble tingkat lanjut.\n",
        "\n",
        "Melalui chapter ini, kita memahami bahwa menggabungkan banyak model yang beragam sering kali menghasilkan sistem yang lebih kuat dibandingkan model tunggal.\n",
        "\n",
        "Pemahaman Ensemble Learning ini menjadi fondasi penting untuk mempelajari model Machine Learning modern yang lebih kompleks dan berperforma tinggi pada chapter berikutnya."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
