{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
    "\n",
    "Bab ini berfokus pada cara menangani dataset besar yang tidak muat di memori (RAM) menggunakan **Data API** (`tf.data`). Kita akan belajar membangun *input pipeline* yang efisien agar GPU/TPU tidak menunggu data dari CPU.\n",
    "\n",
    "## Daftar Isi:\n",
    "1. **The Data API**: Dasar-dasar objek `Dataset`.\n",
    "2. **Transformasi Data**: Chaining, Shuffling, Batching, dan Prefetching.\n",
    "3. **Membaca CSV Skala Besar**: Menggunakan `interleave` untuk file paralel.\n",
    "4. **Format TFRecord**: Menggunakan format biner efisien TensorFlow.\n",
    "5. **Keras Preprocessing Layers**: Normalisasi dan Encoding di dalam Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dasar-Dasar Data API\n",
    "\n",
    "API ini memungkinkan kita membaca data dari memori atau disk dengan cara yang terstandardisasi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Membuat dataset dari Python list\n",
    "X = tf.range(10)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "\n",
    "# Menampilkan isi dataset\n",
    "for item in dataset:\n",
    "    print(item.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Rantai Transformasi (Chaining Transformations)\n",
    "\n",
    "Kita bisa memodifikasi data secara bertahap menggunakan metode *chaining*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengulang dataset 3 kali dan mengelompokkan dalam batch berisi 7\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "dataset = dataset.repeat(3).batch(7)\n",
    "\n",
    "for item in dataset:\n",
    "    print(\"Batch:\", item.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map, Filter, dan Shuffle\n",
    "\n",
    "- **`map`**: Mengubah setiap elemen (misal: normalisasi).\n",
    "- **`filter`**: Memilih elemen tertentu.\n",
    "- **`shuffle`**: Mengacak data agar model tidak menghafal urutan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.range(10)\n",
    "\n",
    "# Map: kalikan dua, Filter: ambil yang > 10, Shuffle: acak dengan buffer\n",
    "dataset = dataset.map(lambda x: x * 2) \\\n",
    "                 .filter(lambda x: x > 10) \\\n",
    "                 .shuffle(buffer_size=5, seed=42) \\\n",
    "                 .batch(3)\n",
    "\n",
    "for item in dataset:\n",
    "    print(\"Filtered & Shuffled Batch:\", item.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Optimasi: Prefetching\n",
    "\n",
    "**Prefetching** memastikan CPU menyiapkan batch $n+1$ saat GPU sedang mengerjakan batch $n$. Ini mengurangi waktu menganggur (idle) pada perangkat keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gunakan tf.data.AUTOTUNE agar TensorFlow menentukan jumlah prefetch optimal\n",
    "dataset = tf.data.Dataset.range(10).batch(3).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Membaca Banyak File CSV (Interleave)\n",
    "\n",
    "Jika data Anda terbagi dalam banyak file CSV, gunakan `interleave` untuk membaca baris dari banyak file secara paralel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_line(line):\n",
    "    # Contoh parsing CSV: 8 fitur float dan 1 label float\n",
    "    defaults = [0.] * 8 + [tf.constant([], dtype=tf.float32)]\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defaults)\n",
    "    return tf.stack(fields[:-1]), fields[-1]\n",
    "\n",
    "# file_paths = [\"file1.csv\", \"file2.csv\", ...]\n",
    "# dataset = tf.data.Dataset.list_files(file_paths)\n",
    "# dataset = dataset.interleave(\n",
    "#     lambda path: tf.data.TextLineDataset(path).skip(1),\n",
    "#     cycle_length=5)\n",
    "# dataset = dataset.map(parse_line, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Format Biner TFRecord\n",
    "\n",
    "**TFRecord** adalah format biner efisien yang menyimpan serialisasi Protocol Buffers. Sangat cepat untuk dibaca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menulis TFRecord\n",
    "with tf.io.TFRecordWriter(\"data_latihan.tfrecord\") as f:\n",
    "    f.write(b\"Data biner pertama\")\n",
    "    f.write(b\"Data biner kedua\")\n",
    "\n",
    "# Membaca TFRecord\n",
    "dataset = tf.data.TFRecordDataset([\"data_latihan.tfrecord\"])\n",
    "for item in dataset:\n",
    "    print(item.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Keras Preprocessing Layers\n",
    "\n",
    "Memasukkan normalisasi dan encoding kategori langsung ke dalam model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "# Layer Normalisasi\n",
    "norm_layer = layers.Normalization()\n",
    "sample_data = np.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], dtype=\"float32\")\n",
    "norm_layer.adapt(sample_data)\n",
    "\n",
    "# Layer StringLookup (untuk kategori)\n",
    "lookup = layers.StringLookup()\n",
    "lookup.adapt([\"Kucing\", \"Anjing\", \"Burung\"])\n",
    "print(\"Encoding 'Anjing':\", lookup(tf.constant([\"Anjing\"])).numpy())\n",
    "\n",
    "# Mengintegrasikan ke dalam model\n",
    "model = tf.keras.models.Sequential([\n",
    "    norm_layer,\n",
    "    layers.Dense(10, activation=\"relu\"),\n",
    "    layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rangkuman Praktis\n",
    "\n",
    "1. **Prefetching**: Gunakan di akhir setiap pipeline data.\n",
    "2. **Parallelize Map**: Gunakan argumen `num_parallel_calls` agar proses CPU lebih cepat.\n",
    "3. **TFRecord**: Gunakan format ini jika dataset Anda sangat masif untuk performa I/O maksimal.\n",
    "4. **Normalization Adapt**: Pastikan memanggil `.adapt()` pada data sampel sebelum melatih model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5

}
