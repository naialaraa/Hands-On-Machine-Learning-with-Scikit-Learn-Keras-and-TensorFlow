{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 5: Support Vector Machines\n",
        "\n",
        "Support Vector Machines (SVM) merupakan salah satu algoritma Machine Learning yang **sangat kuat dan serbaguna**. SVM dapat digunakan untuk classification, regression, hingga outlier detection.\n",
        "\n",
        "Keunggulan utama SVM terletak pada kemampuannya untuk membangun **decision boundary dengan margin terbesar**, sehingga model memiliki kemampuan generalisasi yang baik.\n",
        "\n",
        "Pada chapter ini, kita akan mempelajari:\n",
        "- konsep *large margin classification*,\n",
        "- perbedaan hard margin dan soft margin,\n",
        "- SVM linear dan non-linear,\n",
        "- kernel trick,\n",
        "- SVM untuk regresi,\n",
        "- serta gambaran matematis cara kerja SVM.\n",
        "\n",
        "Pemahaman SVM sangat penting karena konsep margin dan kernel menjadi dasar bagi banyak metode Machine Learning lanjutan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Linear SVM Classification\n",
        "\n",
        "Ide fundamental di balik SVM adalah **large margin classification**, yaitu mencari decision boundary yang tidak hanya memisahkan kelas dengan benar, tetapi juga berada sejauh mungkin dari instance terdekat dari setiap kelas.\n",
        "\n",
        "Decision boundary yang memiliki margin besar cenderung lebih stabil dan lebih mampu melakukan generalisasi terhadap data baru.\n",
        "\n",
        "Instance data yang berada tepat di tepi margin disebut **support vectors**. Menariknya, hanya support vectors inilah yang menentukan posisi decision boundary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Sensitivity to Feature Scales\n",
        "\n",
        "SVM sangat **sensitif terhadap skala fitur**. Jika satu fitur memiliki skala yang jauh lebih besar dibandingkan fitur lain, maka decision boundary dapat menjadi bias.\n",
        "\n",
        "Oleh karena itu, **feature scaling** (misalnya menggunakan StandardScaler) merupakan langkah yang hampir selalu diperlukan sebelum melatih model SVM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Soft Margin Classification\n",
        "\n",
        "Pada kondisi ideal, semua data dapat dipisahkan secara sempurna oleh decision boundary (hard margin). Namun, pada data nyata:\n",
        "- data sering tidak sepenuhnya linearly separable,\n",
        "- terdapat noise dan outlier.\n",
        "\n",
        "Untuk mengatasi hal ini, SVM menggunakan pendekatan **soft margin**, yaitu mengizinkan sebagian pelanggaran margin dengan tujuan mendapatkan model yang lebih robust.\n",
        "\n",
        "Trade-off antara margin yang besar dan jumlah pelanggaran margin dikontrol oleh hyperparameter **C**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Jika nilai **C kecil**, model akan lebih toleran terhadap pelanggaran margin dan cenderung memiliki margin yang lebih besar.\n",
        "\n",
        "Sebaliknya, jika nilai **C besar**, model akan berusaha mengklasifikasikan semua data dengan benar, namun berisiko overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Linear SVM Classification with Scikit-Learn\n",
        "\n",
        "Pada bagian ini, kita akan melatih sebuah Linear SVM untuk melakukan binary classification menggunakan dataset Iris.\n",
        "\n",
        "Tujuan model adalah mendeteksi apakah suatu bunga termasuk spesies **Iris virginica** atau bukan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
        "y = (iris[\"target\"] == 2).astype(np.float64)  # Iris virginica"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "svm_clf = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\")),\n",
        "])\n",
        "\n",
        "svm_clf.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pipeline di atas memastikan bahwa data distandarisasi terlebih dahulu sebelum masuk ke model Linear SVM.\n",
        "\n",
        "Penggunaan `loss=\"hinge\"` sesuai dengan formulasi klasik SVM yang memaksimalkan margin."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Making Predictions\n",
        "\n",
        "Setelah model dilatih, kita dapat menggunakannya untuk melakukan prediksi pada data baru."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "svm_clf.predict([[5.5, 1.7]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hasil prediksi berupa nilai kelas, bukan probabilitas. Berbeda dengan Logistic Regression, SVM **tidak secara langsung menghasilkan probabilitas kelas**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Nonlinear SVM Classification\n",
        "\n",
        "Tidak semua dataset dapat dipisahkan menggunakan decision boundary linear. Pada kasus seperti ini, **Linear SVM** tidak mampu menghasilkan model yang baik.\n",
        "\n",
        "Untuk mengatasi masalah tersebut, SVM dapat diperluas menjadi **Nonlinear SVM**, yaitu dengan cara memproyeksikan data ke ruang berdimensi lebih tinggi sehingga data menjadi *linearly separable* di ruang baru tersebut."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Polynomial Features\n",
        "\n",
        "Salah satu cara paling sederhana untuk menangani data non-linear adalah dengan menambahkan **fitur polinomial**.\n",
        "\n",
        "Dengan menambahkan kombinasi non-linear dari fitur asli, model linear dapat mempelajari decision boundary yang non-linear di ruang input asli."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_moons\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "X, y = make_moons(n_samples=100, noise=0.15, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dataset *moons* sering digunakan sebagai contoh klasik data non-linear. Dua kelas data membentuk pola setengah lingkaran yang tidak dapat dipisahkan oleh garis lurus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "polynomial_svm_clf = Pipeline([\n",
        "    (\"poly_features\", PolynomialFeatures(degree=3)),\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"linear_svc\", LinearSVC(C=10, loss=\"hinge\"))\n",
        "])\n",
        "\n",
        "polynomial_svm_clf.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dengan menambahkan fitur polinomial, model Linear SVM kini mampu memisahkan data non-linear.\n",
        "\n",
        "Namun, pendekatan ini memiliki kelemahan: jumlah fitur meningkat sangat cepat seiring bertambahnya derajat polinomial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. The Kernel Trick (Pengantar)\n",
        "\n",
        "Untuk menghindari eksplisit menambahkan fitur polinomial dalam jumlah besar, SVM menggunakan teknik yang disebut **kernel trick**.\n",
        "\n",
        "Kernel trick memungkinkan kita menghitung hubungan di ruang berdimensi tinggi **tanpa benar-benar memetakan data ke ruang tersebut**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Intinya, kernel trick membuat SVM tetap efisien secara komputasi meskipun decision boundary yang dihasilkan bersifat non-linear di ruang input asli."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pada bagian selanjutnya, kita akan langsung menggunakan kernel SVM yang tersedia di Scikit-Learn, tanpa perlu menambahkan fitur polinomial secara manual."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Polynomial Kernel\n",
        "\n",
        "Alih-alih menambahkan fitur polinomial secara eksplisit, SVM menyediakan **Polynomial Kernel** yang menerapkan *kernel trick*.\n",
        "\n",
        "Dengan polynomial kernel, SVM dapat mempelajari decision boundary non-linear secara efisien, bahkan pada ruang fitur berdimensi sangat tinggi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Polynomial kernel memiliki beberapa hyperparameter penting:\n",
        "- **degree**: menentukan derajat polinomial (kompleksitas decision boundary)\n",
        "- **coef0**: mengontrol pengaruh fitur tingkat rendah vs tingkat tinggi\n",
        "- **C**: mengontrol regularization (trade-off margin vs kesalahan)\n",
        "\n",
        "Pemilihan hyperparameter ini sangat memengaruhi performa dan kompleksitas model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "poly_kernel_svm_clf = SVC(kernel=\"poly\", degree=3, coef0=1, C=5)\n",
        "poly_kernel_svm_clf.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model di atas menggunakan polynomial kernel dengan derajat 3. Dengan kombinasi parameter ini, SVM mampu mempelajari pola non-linear pada dataset *moons*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 Pengaruh Parameter `degree`\n",
        "\n",
        "Parameter `degree` menentukan tingkat kompleksitas decision boundary.\n",
        "\n",
        "- Degree rendah menghasilkan boundary yang lebih sederhana\n",
        "- Degree tinggi memungkinkan boundary yang lebih kompleks, namun meningkatkan risiko overfitting\n",
        "\n",
        "Pemilihan nilai degree yang tepat sangat bergantung pada struktur data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Pengaruh Parameter `coef0`\n",
        "\n",
        "Parameter `coef0` mengontrol kontribusi fitur-fitur tingkat rendah dalam polynomial kernel.\n",
        "\n",
        "Nilai `coef0` yang besar meningkatkan pengaruh fitur tingkat rendah, sedangkan nilai kecil menekankan fitur tingkat tinggi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.3 Pengaruh Parameter `C`\n",
        "\n",
        "Parameter `C` mengatur seberapa ketat model berusaha mengklasifikasikan seluruh data dengan benar.\n",
        "\n",
        "- Nilai `C` kecil menghasilkan margin yang lebih lebar dan model yang lebih sederhana\n",
        "- Nilai `C` besar menghasilkan margin yang lebih sempit dan model yang lebih kompleks\n",
        "\n",
        "Trade-off ini sangat penting untuk menghindari overfitting maupun underfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Gaussian Radial Basis Function (RBF) Kernel\n",
        "\n",
        "Salah satu kernel yang paling populer dan paling sering digunakan dalam SVM adalah **Gaussian Radial Basis Function (RBF) kernel**.\n",
        "\n",
        "RBF kernel memungkinkan SVM membentuk decision boundary yang sangat fleksibel, bahkan pada dataset dengan struktur yang kompleks dan tidak teratur.\n",
        "\n",
        "Secara intuitif, RBF kernel mengukur **kedekatan (similarity)** antara dua instance. Semakin dekat dua titik data, semakin besar nilai kernel-nya."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.1 Intuisi RBF Kernel\n",
        "\n",
        "RBF kernel dapat dibayangkan sebagai cara untuk menempatkan *Gaussian bell* di sekitar setiap instance data.\n",
        "\n",
        "Instance yang berada dekat satu sama lain akan memiliki pengaruh yang kuat, sementara instance yang jauh pengaruhnya akan mengecil.\n",
        "\n",
        "Pendekatan ini membuat SVM dengan RBF kernel mampu mempelajari decision boundary yang sangat non-linear."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.2 Hyperparameter `gamma`\n",
        "\n",
        "Parameter **`gamma`** mengontrol seberapa jauh pengaruh sebuah instance data menjangkau instance lainnya.\n",
        "\n",
        "- **Gamma kecil** → pengaruh instance sangat luas → decision boundary lebih halus\n",
        "- **Gamma besar** → pengaruh instance sangat sempit → decision boundary lebih kompleks\n",
        "\n",
        "Nilai gamma yang terlalu besar dapat menyebabkan overfitting, sedangkan nilai yang terlalu kecil dapat menyebabkan underfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.3 RBF Kernel SVM with Scikit-Learn\n",
        "\n",
        "Scikit-Learn menyediakan implementasi RBF kernel melalui class `SVC`. Dengan memilih kernel `rbf`, kita dapat langsung menggunakan RBF kernel tanpa perlu transformasi fitur manual."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rbf_kernel_svm_clf = SVC(kernel=\"rbf\", gamma=0.5, C=1)\n",
        "rbf_kernel_svm_clf.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model di atas menggunakan RBF kernel dengan nilai `gamma=0.5` dan `C=1`. Kombinasi kedua parameter ini menentukan tingkat fleksibilitas decision boundary.\n",
        "\n",
        "Pada praktiknya, `gamma` dan `C` sering ditentukan menggunakan teknik *hyperparameter tuning* seperti grid search."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.4 Kapan Menggunakan RBF Kernel?\n",
        "\n",
        "RBF kernel merupakan pilihan default yang baik ketika:\n",
        "- hubungan antar fitur bersifat non-linear\n",
        "- jumlah fitur tidak terlalu besar\n",
        "- tidak ada pengetahuan awal tentang bentuk decision boundary\n",
        "\n",
        "Karena fleksibilitasnya, RBF kernel sering memberikan performa yang sangat baik pada berbagai jenis dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Support Vector Machine for Regression (SVR)\n",
        "\n",
        "Selain digunakan untuk classification, Support Vector Machines juga dapat digunakan untuk **regression**, yang dikenal sebagai **Support Vector Regression (SVR)**.\n",
        "\n",
        "Berbeda dengan regresi klasik yang meminimalkan error prediksi secara langsung, SVR berusaha mencari fungsi regresi yang berada dalam batas toleransi tertentu terhadap data training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.1 Intuisi Support Vector Regression\n",
        "\n",
        "Pada SVR, model berusaha memuat sebanyak mungkin instance data **di dalam sebuah margin toleransi** yang ditentukan oleh parameter $\\varepsilon$.\n",
        "\n",
        "- Error yang berada di dalam margin $\\varepsilon$ **tidak dikenakan penalti**\n",
        "- Error di luar margin akan dikenakan penalti\n",
        "\n",
        "Dengan pendekatan ini, SVR fokus pada **kesederhanaan model** dan **robustness terhadap noise**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.2 Linear SVR\n",
        "\n",
        "Linear SVR mencoba menemukan fungsi linear yang memprediksi target dengan deviasi maksimum sebesar $\\varepsilon$.\n",
        "\n",
        "Parameter **C** tetap berperan sebagai pengontrol trade-off antara lebar margin dan penalti terhadap error besar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.svm import LinearSVR\n",
        "\n",
        "svm_reg = LinearSVR(epsilon=1.5)\n",
        "svm_reg.fit(X, y.ravel())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model di atas mencoba memprediksi nilai target dengan mengabaikan error kecil di dalam margin toleransi.\n",
        "\n",
        "Pendekatan ini membuat SVR relatif tahan terhadap noise ringan pada data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.3 Nonlinear SVR\n",
        "\n",
        "Seperti pada classification, SVR juga dapat diperluas menjadi **nonlinear regression** dengan menggunakan kernel, seperti polynomial kernel atau RBF kernel.\n",
        "\n",
        "Hal ini memungkinkan SVR memodelkan hubungan non-linear antara fitur dan target."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.svm import SVR\n",
        "\n",
        "svm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\n",
        "svm_poly_reg.fit(X, y.ravel())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dengan menggunakan polynomial kernel, SVR mampu menangkap pola non-linear pada data regresi.\n",
        "\n",
        "Pemilihan kernel dan hyperparameter sangat memengaruhi performa model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Closing Summary (Chapter 5)\n",
        "\n",
        "Chapter 5 membahas Support Vector Machines sebagai algoritma yang kuat dan fleksibel untuk berbagai jenis tugas Machine Learning.\n",
        "\n",
        "Konsep utama yang dipelajari meliputi:\n",
        "- large margin classification,\n",
        "- soft margin dan regularization,\n",
        "- kernel trick untuk menangani non-linearitas,\n",
        "- serta penerapan SVM pada classification dan regression.\n",
        "\n",
        "Pemahaman SVM memberikan wawasan penting tentang bagaimana model yang kompleks dapat dikontrol agar tetap memiliki kemampuan generalisasi yang baik.\n",
        "\n",
        "Konsep margin, kernel, dan regularization yang diperkenalkan pada chapter ini juga menjadi fondasi penting untuk memahami model Machine Learning lanjutan pada chapter berikutnya."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
